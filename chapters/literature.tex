\chapter{Literature Review}
\label{chapter:literature}

% #TODO Overview of literature review section


\section{Visual Question Answering Metrics}

In their seminal VQA paper, \citeauthor{malinowski2014multiworld} \cite{malinowski2014multiworld} note that traditional accuracy measures fail to capture partial correctness of answers; given a ground-truth answer `carton', the traditional accuracy metric would give zero weight to the answer `box', despite the semantic similarity between the ground-truth and predicted answers. In general, larger answer vocabularies typically lead to blurrier semantic boundaries between answer classes when compared to datasets with a smaller answer vocabulary like CIFAR \cite{krizhevsky2009learning}. Since datasets that frame the VQA problem as a classification task typically have a large answer vocabulary containing objects, numbers, colours and other concepts, it follows that an effective VQA metric should reward answers that are partially correct or share semantics with the ground-truth answer.

Metric design becomes even more important when considering full-sentence answers instead of single-word answers; evaluation of full-sentence answers requires metrics that are robust under semantic and syntactic variations between predicted and ground truth answers. A variety of natural language generation (NLG) metrics \cite{papineni2002bleu, lin2004rouge, banerjee2005meteor, vedantam2015cider} have been applied to image captioning tasks \cite{chen2015microsoft}, and are consequently applicable to open-ended VQA. Unfortunately, each of these metrics have their own advantages and pitfalls, so multiple metrics need to be considered to gain useful insights into model performance.

% Although I focus on VQA as a multi-class classification problem in this dissertation, I provide a brief overview of these metrics in section \ref{subsection:open_ended_vqa_metrics}.

In the following subsections, I explore a variety of metrics for both class-based and free-form VQA tasks as outlined in \tableautorefname{ \ref{tab:vqa_metrics_suitability}}, focusing on how they reward partially correct answers.


\begin{table}[htbp]
    \centering
    \begin{tabularx}{\linewidth}{@{}lCCCl@{}}
        \toprule
        \multicolumn{1}{c}{\multirow{2}{*}{\textbf{Metric}}} & \multicolumn{3}{c}{\textbf{Suitability}}                                                           & \multicolumn{1}{c}{\multirow{2}{*}{\textbf{Common Uses}}} \\ \cmidrule(lr){2-4}
        \multicolumn{1}{c}{}                                 & \multicolumn{1}{c}{Multi-class} & \multicolumn{1}{c}{Multi-label} & \multicolumn{1}{c}{Open-ended} & \multicolumn{1}{c}{}                                      \\ \midrule
        Accuracy                                             & \checkmark                      & \checkmark                      &                                & Multiple                                                  \\
        WUPS                                                 & \checkmark                      & \checkmark                      &                                & Multiple                                                  \\
        Consensus Accuracy                                   & \checkmark                      & \checkmark                      &                                & VQA                                                       \\
        Consistency                                          & \checkmark                      & \checkmark                      &                                & VQA                                                       \\
        Validity                                             & \checkmark                      &                                 &                                & VQA                                                       \\
        Plausibility                                         & \checkmark                      &                                 &                                & VQA                                                       \\
        Grounding                                            & \checkmark                      & \checkmark                      & \checkmark                     & VQA                                                       \\
        Distribution                                         & \checkmark                      &                                 &                                & VQA                                                       \\
        BLEU                                                 &                                 &                                 & \checkmark                     & Machine translation                                       \\
        ROUGE                                                &                                 &                                 & \checkmark                     & Text summarisation                                        \\
        METEOR                                               &                                 &                                 & \checkmark                     & Machine translation                                       \\
        CIDEr                                                &                                 &                                 & \checkmark                     & Image captioning                                          \\ \bottomrule
    \end{tabularx}
    \caption[Suitability of metrics for various VQA tasks]{A comparison of metrics and their suitability for various VQA tasks.}
    \label{tab:vqa_metrics_suitability}
\end{table}

\begin{table}[htbp]
    \begin{footnotesize}
      \begin{tabularx}{\linewidth}{@{}lLL@{}}
        \toprule
            \multicolumn{1}{c}{\textbf{Metric}} & \multicolumn{1}{c}{\textbf{Advantages}}                                                    & \multicolumn{1}{c}{\textbf{Disadvantages}}                                                                                                                              \\ \midrule
            Accuracy                            & - Simple and interpretable.                                                                & - Penalises slightly incorrect answers harshly.                                                                                                                         \\
                                                & - Suitable for problems with a small answer set cardinality.                               & - Requires an exact set of answers for multi-label tasks.                                                                                                               \\ \midrule
            WUPS                                & - Rewards incorrect but semantically similar answers.                                      & - Rewards answers that share a concept class \textit{e.g.} \textit{`colour'} with the true answer but have opposite meanings e.g. \textit{`white'} vs \textit{`black'}. \\
                                                &                                                                                            & - Evaluating open-ended answers ignores sentence syntax and context.                                                                                                    \\ \midrule
            Consensus Accuracy                  & - Simple and interpretable.                                                                & - Multiple correct reference answers may be contradictory.                                                                                                              \\
                                                & - Aligns with human answers.                                                               & - Not all questions have 3 matching reference answers                                                                                                                   \\
                                                &                                                                                            & - Reference answers are expensive to collect.                                                                                                                           \\ \midrule
            Consistency                         & - Ensures models avoid contradictory answers to related questions.                         & - Requires a set of entailed questions for each question.                                                                                                               \\
                                                &                                                                                            & - Limited by the usefulness of the accuracy metric                                                                                                                      \\ \midrule
            Validity                            & - Measures a model's ability to respond to various question types.                         & - Requires a set of valid answers for each question type.                                                                                                               \\ \midrule
            Plausibility                        & - Ensures models avoid nonsensical answers.                                                & - Requires scene graphs for all dataset samples.                                                                                                                        \\
                                                &                                                                                            & - Can be imprecise for sparsely occurring objects.                                                                                                                      \\ \midrule
            Grounding                           & - Ensures models use image data to answer questions instead of exploiting language priors. & - Only applicable to attention-based models.                                                                                                                            \\
                                                &                                                                                            & - Requires knowledge about which image regions are relevant to each dataset sample.                                                                                     \\ \midrule
            Distribution                        & - Ensures models respond well to all question subject-type groups                          &                                                                                                                                                                         \\ \midrule
            BLEU                                & - \(n\)-grams capture positional relationships between words.                              & - Requires multiple ground truth answers for good performance.                                                                                                          \\
                                                & - Penalises answers that are too short or too long.                                        & - Performs better for entire corpora compared to individual sentences due to the corpus-level brevity penalty, an issue for VQA.                                        \\
                                                &                                                                                            & - A zero score for any \(n\)-gram component gives a zero BLEU score.                                                                                                    \\ \midrule
            ROUGE                               & - \(n\)-grams capture positional relationships between words.                              &                                                                                                                                                                         \\ \midrule
            METEOR                              & - Accounts for synonyms                                                                    &                                                                                                                                                                         \\
                                                & - Accounts for morphological discrepancies                                                 &                                                                                                                                                                         \\
                                                & - Accounts for both precision and recall                                                   &                                                                                                                                                                         \\ \bottomrule
    \end{tabularx}
    \end{footnotesize}
    \caption[Advantages and disadvantages of metrics for various VQA tasks]{A comparison of metrics and their advantages and disadvantages for various VQA tasks.}
    \label{tab:vqa_metrics_comparison}
\end{table}

\subsection{Semantic Similarity Measures}

\subsubsection{Wu-Palmer Similarity}

Originally coined the ``conceptual similarity measure'' \cite{wu1994verbs}, the now-labelled Wu-Palmer (WUP) similarity is used to measure the semantic similarity of two concepts based on their positions in a taxonomy tree such as WordNet \cite{miller1995wordnet}. Formally, given two concepts \(A\) and \(B\) in a taxonomy tree \(T\), we define the Wu-Palmer similarity of \(A\) and \(B\) as

\begin{equation}
    WUP(A, B) = \frac{2 N_C}{N_A + N_B + 2 N_C}
    \label{equation:wup}
\end{equation}

where \(N_A\) and \(N_B\) are the number of edges from \(A\) and \(B\) to their lowest common ancestor \(C\) respectively, and \(N_C\) is the depth of \(C\) in \(T\), as illustrated in \figureautorefname{ \ref{fig:wups_tree}}.

\begin{figure}[H]
    \centering
    \begin{forest}
      [ROOT [C, edge=dashed, edge label={node[midway,auto]{\(N_C\)}} [A, edge=dashed, edge label={node[midway,left]{\(N_A\)}}] [B, edge=dashed, edge label={node[midway,right]{\(N_B\)}}]]]
    \end{forest}
    \caption[A taxonomy tree describing the relationship between two concepts.]{Given two concepts \(A\) and \(B\), \(C\) denotes the ``least common superconcept'' of A and B. (Figure adapted from \cite{wu1994verbs})}
    \label{fig:wups_tree}
\end{figure}

Given this definition, it naturally follows that two equal concepts occupy the same position in the taxonomy tree and thus have a similarity of 1, whilst two concepts whose lowest common ancestor is the root of the tree have a similarity of 0.

Whilst WUP similarity can capture similarities between a target and predicted concept, \citeauthor{malinowski2014multiworld} extend WUP similarity to sets of concepts, proposing the WUP set score for evaluation of multi-label classification tasks:

\begin{equation}
    \textsc{WUPS}(A, B) = \frac{1}{N} \sum_{i=1}^N \min \{\prod_{a \in A_i} \max_{b \in B_i} \textsc{WUP}(a, b), \prod_{b \in B_i} \max_{a \in A_i} \textsc{WUP}(a, b)\}
    \label{equation:wups}
\end{equation}

To apply this metric to VQA tasks, we let \(A\) and \(B\) be a sequence of sets of predicted and ground-truth answers respectively, each of size \(N\). Noting that \(WUP(A_i, B_i) \in [0, 1]\) for each \(A_i \in A\) and \(B_i \in B\), the following useful properties of the metric become evident:

\begin{enumerate}
    \item \(WUPS(A, B) \in [0, 1]\), with \(WUPS(A, B) = 1\) when \(A = B\).
    \item Naive models which overestimate the number of target answers will be penalised according to how dissimilar the additional provided answers are to their closest target answer.
    \item Models which underestimate the number of target answers \textit{i.e.} \(|A_i| < |B_i|\) will be penalised, with a harsher penalty if the concepts in the target set are dissimilar.
\end{enumerate}

The authors note that WUP yields large values for related but not interchangeable concepts, noting that \(WUP(\text{carton}, \text{box}) = 0.94\) where \(WUP(\text{stove}, \text{fire extinguisher}) = 0.82\). To encourage precise answers and avoid accepting related but inaccurate answers, the authors down-weight \(WUP(a, b)\) by a ratio \(r = 0.1\) whenever it is less than some threshold \(t = 0.9\).

While this down-weighting alleviates this issue for questions that query objects in an image, answers to questions about object attributes and colours are often poorly evaluated; \citeauthor{kafle2017visual} note that even after down-weighting \(WUP\) scores, \(WUPS(\text{white}, \text{black}) = 0.91\). Whilst the metric correctly rewards the answerer for correctly identifying that the answer to the question is a colour, a score of 0.91 seems too high, highlighting the non-linearity of the metric that makes it more difficult to interpret at a glance unlike other common metrics like accuracy.

By nature, the metric also operates on single WordNet concepts, which often coincide with single words. To evaluate full-sentence or even multi-word answers with WUPS, we must treat sentences as a set of distinct words, disregarding essential syntactic and contextual information. This makes WUPS a poor candidate for open-ended VQA evaluation when compared to other NLG metrics such as BLEU, ROUGE, METEOR and CIDEr, as briefly discussed in \subsectionautorefname{ \ref{subsection:open_ended_vqa_metrics}} and summarised in \tableautorefname{ \ref{tab:vqa_metrics_comparison}}.

%In addition to this issue, the down-weighting approach introduces two new parameters which increase the complexity and subjectivity of the metric. 


% \[
% \min \{\prod_{a \in A_i} \max_{b \in B_i} WUP(a, b), \prod_{b \in B_i} \max_{a \in A_i} WUP(a, b)\} =  \begin{cases}
%     1, &\text{if } A_i = B_i \\
%     \prod_{b \in B_i} \max_{a \in A_i} WUP(a, b) < 1, &\text{if } A_i \subset B_i \\
%     \prod_{a \in A_i} \max_{b \in B_i} WUP(a, b) < 1, &\text{if } A_i \supset B_i
% \end{cases}
% \]

\subsection{Consensus Measures}
\label{subsection:consensus_measures}

\subsubsection{VQA Consensus Accuracy}
Introduced by \citeauthor{antol2015vqa} in \citeyear{antol2015vqa}, the VQA consensus accuracy metric is the most common metric used to benchmark VQA models, due to the widespread adoption of the VQA dataset and its variants. To reward partially correct answers, VQA consensus accuracy compares a candidate answer to multiple reference answers, provided by humans via crowdsourcing platforms like Amazon Mechanical Turk (AMT) in the case of the VQA dataset. The VQA consensus accuracy metric is defined as

\begin{equation}
    \textsc{Accuracy} = \min \left( \frac{n}{3}, 1 \right)
    \label{equation:vqa_consensus_accuracy}
\end{equation}

where \(n\) is the number of answers in the set of reference answers that match a given candidate answer, \textit{i.e.} an answer is considered entirely correct if it matches 3 or more of the 10 reference answers for the corresponding question and image.

At a surface level, the metric is interpretable and can reward multiple correct and/or partially correct answers effectively, however, the latter can become a disadvantage depending on the quality of the reference answers. In their analysis of the VQAv1 dataset, \citeauthor{kafle2017visual} point out that 16.7\% of questions have less than 3 matching answers in their group of 10 reference answers, capping the performance of a perfect VQA model. A large contributor to this statistic is the presence of multi-word answers in the dataset; 89.32\% of answers in the VQAv1 dataset are single-word answers, leaving ample room for disagreement amongst reference answers.

Moreover, supporting multiple correct answers is unfavourable if the correct answers are contradictory. Again, \citeauthor{kafle2017visual} identify that 13\% of VQA v1 yes/no questions contain both `yes' and `no' 3 or more times in their group of 10 reference answers, making both options entirely correct.


% \begin{itemize}
%     \item ``Inter-human agreement is only 83.3\%. It is impossible for an algorithm to achieve 100\% accuracy.''\cite{kafle2017visual} % no consensus answer problem
%     \item even when counting only those answers in the VQAv2 training set that appear more than 8 times, there are still 3129 unique reference answers. After this selection,  \cite{teney2018tips}.
% \end{itemize}

\subsubsection{DAQUAR Consensus Measures}

Motivated by the relatively poor human baseline performance (Accuracy: 50.20\%, WUPS@0.9:  50.82\%) on the DAQUAR dataset, \citeauthor{malinowski2015ask} extended the dataset by collecting an average of 5 reference answers for each image-question pair and developing two consensus metrics based on their previous WUPS measure:

\begin{equation}
    \textsc{WUPS}_{\text{mean}}(A, B) =     \frac{1}{NK} \sum_{i=1}^N \sum_{i=1}^K \min \{\prod_{a \in A_i} \max_{b \in B_i} \textsc{WUP}(a, b), \prod_{b \in B_i} \max_{a \in A_i} \textsc{WUP}(a, b)\}
    \label{equation:wups_mean_consensus}
\end{equation}


\begin{equation}
    \textsc{WUPS}_{\text{min}}(A, B) =     \frac{1}{N} \sum_{i=1}^N \max_{k=1}^K \left( \min \{\prod_{a \in A_i} \max_{b \in B_i} \textsc{WUP}(a, b), \prod_{b \in B_i} \max_{a \in A_i} \textsc{WUP}(a, b)\} \right)
    \label{equation:wups_min_consensus}
\end{equation}

where \(N\), \(A\) and \(B\) are as defined in \equationautorefname{ \ref{equation:wups}}, and \(K\) is the number of reference answers for the \(i\)-th dataset sample.

The mean consensus metric rewards candidate answers that coincide with more frequent reference answers, whilst the minimum consensus metric rewards candidate answers according to their similarity to the closest reference answer.

\subsection{Metrics for Visual Reasoning}

As discussed earlier, the standard accuracy metric penalises partially correct answers as harshly as incorrect answers. When using accuracy alone, it is impossible to distinguish between a model that provides reasonable but incorrect answers and one that provides nonsensical answers. Moreover, the standard accuracy metric gives no insights into how a VQA model arrived at its answer to a given question and image.

To combat these issues, \citeauthor{hudson2019gqa} developed five new metrics in tandem with their GQA dataset \cite{hudson2019gqa} to evaluate the internal reasoning processes of VQA models: Consistency, Validity, Plausibility, Grounding and Distribution.

\subsubsection{Consistency}

The consistency metric is motivated by the following scenario:

Say we have two questions \(q_0\) and \(q_1\) that ask about properties of the same image, \textit{e.g.} \(q_0 = \)\textit{`What colour is the apple?'} and  \(q_1 = \) \textit{`Is the apple red?'}

Since both questions are asking about the same apple, a consistent model is one that does not contradict itself in answering the two questions \textit{e.g.} \(a_0 =\) \textit{`red'} and \(a_1 =\) \textit{`yes'} are consistent answers, where \(a_0 =\) \textit{`red'} and \(a_1 =\) \textit{`no'} are not.

To calculate consistency, each question-answer pair \(q, a\) in the GQA dataset is annotated with a set of entailed questions \(E_q\). For the example above, \(q_1\) is an entailed question of \(q_0\), since knowing the answer to \(q_0\) infers the answer to \(q_1\). Given a the set of questions \(Q\) that a model answered correctly and a set entailed questions for each \(q \in Q\) denoted \(\mathcal{E} = \{E_q \mid q \in Q\}\), the consistency metric is calculated according to \algorithmcfname{ \ref{algorithm:consistency_metric}}.

\begin{algorithm}[htbp]
    \Function{\(Q, \mathcal{E}\)}{
        \(s_c \leftarrow 0\)\\
        \ForEach{\(\{q_i \in Q \mid E_{q_i} \neq \emptyset\}\)}{
            \(s_{q_i} \leftarrow 0\)\\
            \ForEach{
                \(q_e \in E_{q_i}\)
            }{
                \lIf{
                    \(q_e \in Q\)
                }{
                    \(s_{q_i} \leftarrow s_{q_i} + 1\)
                }
            }
            \(s_c \leftarrow s_c + \frac{s_{q_i}}{|E_{q_i}|}\)
        }
        \Return \(\frac{s_c}{|\{q_i \in Q \mid E_{q_i} \neq \emptyset\}|}\)
    }
    \caption[Consistency metric algorithm]{Consistency metric algorithm}
    \label{algorithm:consistency}
\end{algorithm}

% On 4000 random questions of the GQA dataset, \citeauthor{hudson2019gqa} report a human consistency of 98.4, and accuracy of 89.3, indicating that humans are mostly consistent but not always accurate in their responses.

Unfortunately, the consistency metric does not account for inaccurately answered questions. There is certainly an argument to be made that including incorrect answers in the consistency calculation would skew the consistency metric in favour of more accurate models. Moreover, including these incorrect answers could give large consistency scores for poorer models that exploit statistical priors in the dataset's answer distribution.
Nonetheless, knowing consistency amongst incorrectly answered questions gives information about the model's overall reasoning skills and tendency to exploit statistical biases in certain question types. Hence, for error analysis purposes, it would be more suitable to compute consistency over incorrectly answered questions instead of computing consistency over all questions.

% Multi-question learning?

\subsubsection{Validity}

The validity metric is the most lenient of the GQA metrics, and simply captures whether a candidate answer corresponds with the type of answer the question demands. For example, valid answers for questions starting with `What colour' are colours like `red' or `blue', where answers like `yes' or `three' would be invalid.

To calculate validity, the GQA evaluation script \cite{hudson2019evaluation_script} contains a set of valid answers \(V_q\) for every question in the dataset. For each question \(q \in Q\) and its corresponding candidate answer \(\hat{a}_q\), the validity metric is simply:

\begin{equation}
    \textsc{Validity} = \frac{1}{|Q|}\sum_{q \in Q} \begin{cases}
        1, &\text{if } \hat{a}_q \in V_q\\
        0, &\text{otherwise }\\
    \end{cases}
    \label{equation:validity}
\end{equation}

\subsubsection{Plausibility}

Where the validity metric aims to measure whether answers to questions are non-degenerate, plausibility measures whether the candidate answer is rational by checking for co-occurrence of the answer and the question's subject in the scene graphs for the rest of the dataset.

In practice, plausibility is computed in the same way as validity, using a pre-computed set of plausible answers \(P_q\)  for each question in place of the set of valid answers \(V_q\) used in \equationautorefname{ \ref{equation:validity}}.

The method used to compute \(P_q\) for each question depends on the question's type; for the GQA dataset, the scene graph contains attributes, objects and relations, meaning that plausibility needs to be computed differently depending on whether the question's answer is an object, relation or attribute. Moreover, plausibility also needs to be computed for other types of questions whose answers are not in the scene graph, such as binary and counting type questions.

For the sake of brevity, I will focus on two different question types to give an intuition about how sets of plausible answers are determined.

For questions that ask about an attribute of an object \(o\), an attribute is considered plausible if it co-occurs with \(o\) at least once in the scene graphs for other images in the dataset. For example, `red' would be a plausible answer to the question `What colour is the apple?' if the dataset contains at least one image of a red apple, where `blue' would be implausible.

For questions that ask about a target object \(t\) given a subject \(s\) and relation \(r\) \textit{e.g.} `What is the man holding?', \(t\) is considered plausible if there exists a relation \(s, r, t\) in the scene graph for any image in the dataset. 

\subsubsection{Grounding}

\(D\): dataset
\(\mathcal{R}\): sets of region pointers for each question image pair in \(D\)
\(\hat{\mathcal{R}}\): attended image regions for each question image pair in \(D\)

\(\mathcal{R} = \{R_{(q,i)} \mid (q,i) \in D\}\)
\(\hat{\mathcal{R}} = \{\hat{R}_{(q,i)} \mid (q,i) \in D\}\)


score = 0
for each region in \(\mathcal{R}\):
    for each region in \(\hat{\mathcal{R}}\):
        score += attention(\(\hat{\mathcal{R}}\)) * intersectionRate(\(\hat{\mathcal{R}}\), \(\mathcal{R}\))
        where intersectionRate(A, B) = Intersection(A, B) / Area(A) i.e. precision-based measure.

Average the above over all samples

\begin{algorithm}[htbp]
    \Function{\(D, \mathcal{R}\)}{

    }
    \caption[Grounding metric algorithm]{Grounding metric algorithm}
    \label{algorithm:grounding}
\end{algorithm}

\subsubsection{Distribution}

Given the importance of dataset balancing and the influence of language priors on model performance as discussed in \sectionautorefname{ \ref{section:vqa_datasets}}, the distribution metric compares the distributions of reference and candidate answer sets. This provides a way of measuring whether a VQA model exploits statistical priors and focuses on more common answers or whether it can predict less-frequent answers based on its knowledge of the question and image.

The similarity score of the candidate and reference distributions for each question type is computed using the Pearson Chi-squared statistic \cite{pearson1900x}, and the distribution metric is a weighted average of these scores, with the weights being the number of questions of that type.

{\color{red} TODO: Insert formula written down on GQA paper (derived from the eval code)}

\subsection{Metrics for Open-ended VQA}
\label{subsection:open_ended_vqa_metrics}

Given the challenge of multi-class and multi-label VQA tasks, there has been little research into open-ended VQA task, even though some datasets contain full-sentence answers for each question. In addition, research into open-ended VQA has been stunted by the lack of a widely accepted metric for the task; metrics such as BLEU,  ROUGE, METEOR and CIDEr were originally developed for other tasks as shown in \tableautorefname{ \ref{tab:vqa_metrics_comparison}}, and thus have various disadvantages when used in VQA contexts.

BLEU and METEOR were originally developed for automatic evaluation of machine translation systems.

{\color{red} require multiple reference translations for good functionality \(\rightarrow\) Missing from VQA datasets}

\subsubsection{BLEU}

\textbf{Motivations:}

\begin{itemize}
    \item Machine translation
    \item Human evaluation can take months
    \item Bi-Lingual Evaluation Understudy (BLEU) is:
    \begin{itemize}
        \item ``Quick''
        \item ``Inexpensive''
        \item ``Language independent''
        \item Accurate (``Correlates highly with human evaluation'')
    \end{itemize}
    \item Created in an effort to reduce the evaluation bottleneck on machine translation progress and promote more rapid prototyping of ideas.
    \item Derived from word-error-rate metric used in speech recognition, modified to allow for subtle word reorderings and substitutions.
\end{itemize}


\textbf{General Caveats:}

\begin{itemize}
    \item Relies on multiple ground truth sentences with different flavours/styles (translations for NMT, answers for VQA or image captioning). BLEU does better on a large corpus than for individual sentences.
    \item Performs better for entire corpora compared to individual sentences due to the corpus-level brevity penalty.
    \item Brevity penalty is required to avoid short answers like ``of the'', where there is only one \(2\)-gram which is likely to occur in ground truth answers but contains no useful contextual information.
\end{itemize}


\textbf{Application to VQA:}

Brevity penalty in the context of VQA:

{\color{red} TODO: Check image captioning paper and see if this is the adopted approach. Also check DAQUAR and COCO-QA papers.} Assume a candidate answer may contain multiple sentences and a ground truth answer may contain multiple sentences. In the most common case, the number of sentences in both the candidate and ground truth answers is 1. For cases with multiple sentences we can treat multiple sentences as a single sentence when computing \(n\)-grams, as in \cite{papineni2002bleu}. For a candidate answer of length \(c\) and a reference answer length of \(r\) (for each candidate sentence, the reference length is the corresponding ground-truth sentence with the closest length. \(r\) is the sum of these reference lengths across all sentences in the corpus, though the corpus is small for VQA applications.) The brevity penalty \(P_B\) is thus:

\begin{equation*}
    P_B = \begin{cases}
    1, &\text{if } c > r\\
    e^{\frac{1-r}{c}}, &\text{if } c \leq r\\
    \end{cases}
\end{equation*}


By default, BLEU is defined as the brevity penalty multiplied by the geometric mean of \(n\)-gram precisions \(p_n = \frac{\sum{c \in C} \sum g_n \in c }{\sum{c' \in C} \sum g_n' \in c}\) for \(n \in [1, 4]\):

\begin{equation*}
    \textsc{BLEU} = P_B \cdot \exp \left(\frac{1}{N} \sum_{n=1}^N \log p_n\right)
\end{equation*}

\begin{itemize}
    \item VQA datasets rarely contain open ended answers. COCO comes with 5 captions per image, GQA comes with 1 open-ended answer per question. Even when we do have open ended answers, a single, one sentence is not enough for BLEU to be successful since it is designed for large corpora with multiple source translations; If the ground truth answer refers to a ``carton'' but the predicted answer uses the word ``box'', \(n\)-gram precision decreases despite semantic similarities between the two words. 
\end{itemize}

\subsubsection{ROUGE}

\textbf{Motivations:}

\begin{itemize}
    \item Recall based rather than precision-based like BLEU. For text summarisation we generally prefer a candidate summary that covers a consensus of ground-truth answers.
    \item Different variations:
    \begin{itemize}
        \item \textsc{Rouge-N}: ``\(n\)-gram recall between a candidate summary and set of reference summaries''
    \end{itemize}
\end{itemize}

\textbf{General Caveats:}

\begin{itemize}
    \item 
\end{itemize}

\textbf{Application to VQA:}

Assume the following notation:

\(S\) is a set of ground-truth summaries (answers in the context of VQA) for a given dataset sample.

\(c\) is a candidate summary, \textit{e.g.} one produced by a model.

\(g_n\) is an \(n\) gram contained within a single summary \(s \in S\). In practice, each summary may contain multiple sentences, in which case \(n\)-grams are not computed across sentence boundaries, but for open-ended VQA answers, we can assume the answer is short and can be expressed as a single sentence.

\begin{equation*}
    \textsc{ROUGE -N} = {\arg \max}_{s\in S} \frac{
        \sum_{g_n \in s} \min \left(\text{Count}_s(g_n), \text{Count}_c(g_n) \right)
    }{
        \sum_{g_n \in s} \text{Count}_s(g_n)
    }
\end{equation*}

For VQA with a single one-sentence reference answer and one-sentence candidate answer, \textsc{Rouge-N} is simply \(n\)-gram recall.

\subsubsection{METEOR}

\textbf{Motivations:}

\begin{itemize}
    \item 
\end{itemize}

\textbf{General Caveats:}

\begin{itemize}
    \item 
\end{itemize}

\textbf{Application to VQA:}
\begin{itemize}
    \item 
\end{itemize}

\subsubsection{CIDEr}

\textbf{Motivations:}

\begin{itemize}
    \item 
\end{itemize}

\textbf{General Caveats:}

\begin{itemize}
    \item 
\end{itemize}

\textbf{Application to VQA:}
\begin{itemize}
    \item 
\end{itemize}


\section{Visual Question Answering Datasets}
\label{section:vqa_datasets}

In this section, I address the first major hurdle for the VQA task as introduced in \chapterautorefname{  \ref{chapter:introduction}}: How do we design a dataset that effectively emulates subtle relationships between questions and images as presented in real-world situations where VQA would prove useful?

% More formally, we wish to develop a dataset \(\mathcal{X}\) such that \(x \sim \mathcal{D}\ \forall x \in \mathcal{X}\) for some fixed, underlying distribution \(\mathcal{D}\) \cite{mohri2018foundations}. For a dataset to be \textit{useful}, we require that \(\mathcal{D}\) captures the distribution of real-world VQA problems that we wish to solve.
%\(S \in ( \times \mathcal{Y})^n\) 

% The methods used to generate VQA datasets are as useful in understanding the relationship between images and questions as VQA models themselves; as an example, it is much easier to generate questions instead of images, the images for many VQA datasets have been drawn from existing image datasets such as COCO \cite{lin2014microsoft}, and questions have been generated using a variety of methods including crowd-sourcing, question templates, functional programs and scene graphs. By analysing the question generation process, it is possible to gain insights into how graphical and textual data is related in a question-answering context, and use this knowledge to analyse the effectiveness of the ways in which image and question data is processed for use by various VQA models. An overview of the most popular VQA datasets and their generation methods is provided in Table \ref{tab:dataset_comparison}, followed by a detailed deconstruction of the methods used and challenges addressed by their respective creators and how they relate to the interplay of images and text in visual reasoning tasks.


% TODO merge with table
% \begin{table}[H]
%   \centering
%   \begin{tabular}{|p{0.15\linewidth}|c|p{0.1\linewidth}|p{0.1\linewidth}|p{0.4\linewidth}|}
%       \hline
%       \textbf{Dataset}                            & \textbf{Images} & \textbf{Questions} & \textbf{Answer Type} & \textbf{Creation Processes} \\
%       \hline
%       \textbf{VQA 1.0} \cite{antol2015vqa}                  & 204K$^\dagger$ & 0.61M & Open-ended, multiple choice & Real images were drawn from COCO \cite{lin2014microsoft}, and abstract scenes were created using clip-art objects \cite{zitnick2013bringing}. Questions and answers were collected using an Amazon Mechanical Turk (AMT) interface.\\
%       \textbf{Visual Genome (VG)} \cite{krishna2017visual}      & 108K & 1.7M & Open-ended & Images from the intersection of COCO \cite{lin2014microsoft} and YFCC100M \cite{thomee2016yfcc100m}. Questions, answers and scene graphs were collected using an AMT interface.\\
%       \textbf{Visual7W} \cite{zhu2016visual7w}             & 47K & 0.32M & Multiple choice & Images are a subset of the Visual Genome dataset. Questions, answers and object-level groundings were collected using an AMT interface.\\
%       \textbf{VQA 2.0} \cite{goyal2017making, zhang2016yin} & 204K$^\dagger$ & 1.6M & Open-ended, multiple choice & Built on top of VQA 1.0, new (image, question, answer) tuples were created from existing tuples by asking AMT workers to choose an image from 24 nearest neighbours of the existing tuple's image such that the answer to the new image is not the answer to the existing image for the same question.\\
%       \textbf{CLEVR} \cite{johnson2017clevr}               & 100K & 0.85M Unique & Open-ended & Images were generated randomly from a scene graph containing objects, attributes and relations between geometric objects. Questions and answers were generated from functional program templates evaluated on the scene graph for a given image. Scene graphs and functional programs were retained and are included for the training and validation sets.\\
%       \hline
%   \end{tabular}
% \end{table}

% \begin{table}[H]
%   \centering
%   \begin{tabular}{|p{0.15\linewidth}|c|p{0.1\linewidth}|p{0.1\linewidth}|p{0.4\linewidth}|}
%       \hline
%       \textbf{TDIUC} \cite{kafle2017analysis}              & 167K & 1.6M & Open-ended & Questions were collected from COCO-QA and VG, generated from COCO and VG segmentation and object/attribute annotations respectively, or created by human volunteers. Images from COCO and VG were used if associated with a question. \\
%       \textbf{GQA} \cite{hudson2019gqa}                    & 113K & ~22M Full, 1.7M Balanced & Open Ended, with a vocabulary of 3097 words and 1878 unique answers. & Scene graphs were created using various preprocessing techniques from VG scene graphs. Questions were produced compositionally from combinations of templates derived from VQA 2.0 and manually-created templates, using scene graph traversals to fill in subjects, objects, relations and attributes, whilst simultaneously filling in the functional program templates associated with the question types. An additional balancing helped mitigate statistical priors that are often exploited by models, whilst maintaining real-world priors.\\
%       \hline
%   \end{tabular}
%   \caption{A comparison of relevant features of the most popular VQA datasets. Datasets are listed in chronological order from top to bottom. $^\dagger$Real-image dataset only.}
%   \label{tab:dataset_comparison}
% \end{table}

\begin{landscape}
  \begin{footnotesize}
      \begin{center}
        \begin{longtable}[c]{llllllllll}
          \toprule
          \textbf{Dataset}
          & \textbf{Year}
          & \textbf{Images}
          & \textbf{Questions}
          & \textbf{Image Source}
          & \textbf{Question Creation}
          & \textbf{Question Category Count}
          & \textbf{Answer Type}
          & \textbf{Additional Data}
          & \textbf{Evaluation Metrics}\\
          \midrule
          \endhead

          \midrule
          \endfoot

          \bottomrule\\
          \caption[A comparison of VQA datasets and their variations.]{A comparison of relevant features of the most popular VQA datasets. Dataset variations are listed in regular font below their bolded counterparts.}
          \label{tab:dataset_comparison}
          \endlastfoot

          \textbf{DAQUAR} \cite{malinowski2014multiworld}
          & 2014
          & 1,449  % 1449 images
          & 12,468  % 12,468 questions
          & NYU-Depth V2 \cite{silberman2012indoor}
          & Both
          & 4
          & Multi-label (894)
          & Human segmentation, image depth values, and object labels from NYU Depth V2.
          & Accuracy, WUPS \\
          \textbf{Visual Madlibs} \cite{yu2015visual}
          & 2015
          & 10,738  % 10,738 images  % Filtered hard task?
          & 360,001  % 360,001 descriptions
          & COCO \cite{lin2014microsoft}
          & Human
          & 12
          & Fill-in-the-blank, Multi-choice (4)
          & -
          & Accuracy, BLEU \\
          \textbf{COCO-QA}
          & 2015
          & 123,287
          & 117,684
          & COCO
          & Automatic
          & 4
          & Single-word
          & -
          & Accuracy, BLEU \\
          \textbf{VQAv1} \cite{antol2015vqa}
          & 2015
          & 204K  % 204,721 images
          & 614K  % 614,163 questions % 6,141,630 ground truth answers % 1,842,489 plausible answers
          & COCO
          & Human
          & -
          & Open-ended, Multi-choice (18)
          & COCO image captions
          & Accuracy\footnotemark \\
          Abstract Scenes
          & 2015
          & 50K  % 50,000 abstract scenes
          & 150K  % 150,000 questions % 1,500,000 ground truth answers % 450,000 plausible answers % 250,000 captions
          & Clip art, 2D
          & Human
          & -
          & Open-ended, Multi-choice (18)
          & Image captions
          & Accuracy\footnotemark[\value{footnote}] \\
          Changing Priors (CP) \cite{agrawal2018dont}
          & 2018
          & {\color{red}\(\approx\)204K}  % Exact unkown as calculation was based on sum of train and test which are not necessarily mutually exclusive on images.
          & {\color{red}\(\approx\)370K}  % Exact unkown, ~ 3.8M answers
          & COCO
          & Human
          & -
          & Open-ended
          & See VQAv1
          & Accuracy\footnotemark[\value{footnote}] \\
          Compositional VQA (C-VQA) \cite{agrawal2017cvqa}
          & 2017
          & 204K  % Some images repeated in train and test
          & 369K  % 369,861 questions, 3,698,610 answers
          & COCO
          & Human
          & -
          & Open-ended
          & See VQAv1
          & Accuracy\footnotemark[\value{footnote}] \\
          \textbf{VQAv2} \cite{goyal2017making}
          & 2017
          & 204K  % 204,721 images
          & 1.1M  % 1,105,904 questions % 11,059,040 ground truth answers
          & COCO
          & Human
          & -
          & Open-ended
          & COCO image captions, Complementary image pairs
          & Accuracy\footnotemark[\value{footnote}] \\
          Balanced Binary Abstract Scenes \cite{zhang2016yin}
          & 2016
          & 31K % 31,325 abstract scenes
          & 33K % 33,383 questions % 333,830 ground truth answers
          & Clip art, 2D
          & Human
          & -
          & Multi-choice (2)
          & Image captions
          & Accuracy\footnotemark[\value{footnote}] \\
          Changing Priors (CP) \cite{agrawal2018dont}
          & 2018
          & {\color{red}\(\approx\)219K}  % Exact unkown as calculation was based on sum of train and test which are not necessarily mutually exclusive on images.
          & {\color{red}\(\approx\)658K}  % Exact unkown, ~ 6.6M Answers
          & COCO
          & Human
          & -
          & Open-ended
          & See VQAv2
          & Accuracy\footnotemark[\value{footnote}] \\
          \textbf{Visual Genome} \cite{krishna2017visual}
          & 2016
          & 108K  % 108,077 images
          & 1.7M  % 1,773,258 questions and answers
          & COCO, YFCC100M \cite{thomee2016yfcc100m}
          & Human
          & -
          & Open-ended
          & COCO annotations, Region descriptions, Scene graphs
          & Accuracy \\
          \textbf{Visual7W} \cite{zhu2016visual7w}
          & 2016
          & 47K % 47,300
          & 327K % 327,939
          & COCO
          & Human
          & -
          & -
          & - \\
          \textbf{TDIUC} \cite{kafle2017analysis}
          & 2017
          & 167K % 167,437 images
          & 1.6M % 1,654,167 questions
          & COCO, Visual Genome
          & Both
          & -
          & Open-ended
          & -
          & Per-question-type accuracy, regular \& normalised arithmetic \& harmonic mean accuracy\\
          \textbf{CLEVR} \cite{johnson2017clevr}
          & 2017
          & 100K  % 100,000 images
          & 999K  % 999,968 questions
          & Computer-generated, 3D
          & Automatic
          & -
          & Open-ended
          & Functional programs, Scene graphs
          & Accuracy \\
          CoGenT-A \& B
          & 2017
          & 100K
          & 999K
          & Computer-generated, 3D 
          & Automatic
          & -
          & Open-ended
          & Functional programs, Scene graphs
          & Accuracy \\
          Humans
          & 2017
          & -
          & 32K  % 32,164
          & CLEVR
          & Human
          & Open-ended
          & See CLEVR
          & Accuracy \\
          \textbf{GQA} \cite{hudson2019gqa}
          & 2019
          & 113K  % 113,018 images
          & 22.6M  % 22,669,678 questions
          & -
          & Both
          & -
          & Open-ended
          & Scene graphs, Functional programs, Full-sentence answers
          & Accuracy, Consistency, Validity, Plausibility, Distribution, Grounding \\
          %  & - & - & - \\
          % \textbf{V7W} \cite{zhu2016visual7w} & - & 47K & 0.32M & Multiple choice \\
          % \textbf{TDIUC} \cite{kafle2017analysis} & - & 167K & 1.6M & Open-ended \\
          \textbf{TextVQA} \cite{singh2019towards}
          & -
          & -
          & -
          & -
          & -
          & -
          & -
          & -
          & Accuracy, BLEU \\
          \end{longtable}
      \end{center}
  \end{footnotesize}
\end{landscape}

\footnotetext{For open-ended answers, an answer is considered `correct' if it matches at least three of the ten human-provided answers. For multiple choice answers, a traditional accuracy metric is used.}

\textbf{VQAv1} \cite{antol2015vqa} Real images were drawn from COCO \cite{lin2014microsoft}, and abstract scenes were created using clip-art objects \cite{zitnick2013bringing}. Questions and answers were collected using an Amazon Mechanical Turk (AMT) interface.

\textbf{VG} \cite{krishna2017visual} Images from the intersection of COCO \cite{lin2014microsoft} and YFCC100M \cite{thomee2016yfcc100m}. Questions, answers and scene graphs were collected using an AMT interface.

\textbf{V7W} \cite{zhu2016visual7w} Images are a subset of the Visual Genome dataset. Questions, answers and object-level groundings were collected using an AMT interface.

\textbf{VQAv2} \cite{goyal2017making, zhang2016yin} Built on top of VQA 1.0, new (image, question, answer) tuples were created from existing tuples by asking AMT workers to choose an image from 24 nearest neighbours of the existing tuple's image such that the answer to the new image is not the answer to the existing image for the same question.

\textbf{CLEVR} \cite{johnson2017clevr} Images were generated randomly from a scene graph containing objects, attributes and relations between geometric objects. Questions and answers were generated from functional program templates evaluated on the scene graph for a given image. Scene graphs and functional programs were retained and are included for the training and validation sets.

\textbf{TDIUC} \cite{kafle2017analysis} Questions were collected from COCO-QA and VG, generated from COCO and VG segmentation and object/attribute annotations respectively, or created by human volunteers. Images from COCO and VG were used if associated with a question.

\textbf{GQA} \cite{hudson2019gqa} Scene graphs were created using various preprocessing techniques from VG scene graphs. Questions were produced compositionally from combinations of templates derived from VQA 2.0 and manually-created templates, using scene graph traversals to fill in subjects, objects, relations and attributes, whilst simultaneously filling in the functional program templates associated with the question types. An additional balancing helped mitigate statistical priors that are often exploited by models, whilst maintaining real-world priors.


\subsection{Early Work} 

Given the recent rise in popularity of Visual Question Answering tasks, \citeauthor{antol2015vqa} recognised a need for a new, sufficiently large dataset that contained free-form, open-ended questions that still allowed results to be quantified; this need was realised by two datasets: an open-ended dataset derived from COCO \cite{lin2014microsoft}, as well as a smaller synthetic dataset with similarly created questions.\cite{antol2015vqa} The key contributions of these datasets were two-fold:

\begin{enumerate}
    \item The real-world image dataset provided the first feasible dataset for benchmarking VQA models, thanks to its large size compared to existing VQA datasets like DAQUAR \cite{malinowski2014multiworld} and COCO-QA \cite{ren2015exploring}. Whilst smaller, the synthetic dataset allowed researchers to investigate the underlying theory behind visual reasoning without focusing on the difficult task of extracting features from images.
    \item Both datasets employed a new open-ended accuracy metric that rewarded models for feasible answers based on answer distributions generated from human answers to questions; an answer is considered ``100\% accurate if at least 3 workers provided that exact answer.'' \citeauthor{antol2015vqa}. Whilst not a perfect metric due to the coarseness of the collected human answer distributions, the authors justify that metrics such as BLEU and ROUGE from other NLP tasks are not suited to the dataset, and thus promote the importance of further research into new metrics for VQA tasks.
\end{enumerate}


\subsection{Mitigating the Exploitation of Language Priors} 

Despite VQA 1.0's strengths, the authors identify the first main hurdle for VQA datasets, highlighting that if left to their own devices, VQA models will exploit statistical priors present in textual information. They describe how language-only methods perform surprisingly well compared to models that harness both question and image features, noting that a simple ``per-question type prior'' model achieves 71.03\% and 35.77\% on binary and numeric questions, and an LSTM with only question word embeddings performs similarly at 78.20\% and 35.68\%  \cite{antol2015vqa}. Further studies \cite{goyal2017making, zhang2016yin} reinforce these observations, highlighting the skewed answer distributions of both the real-world and synthetic VQA datasets, demonstrating that models could leverage language priors to effectively answer some types of questions without requiring any knowledge of associated images. \citeauthor{zhang2016yin} focus on the imbalance in the answer distribution of binary questions in the VQA dataset, pointing out that `yes' is the answer to 68\% of all binary questions in the abstract scenes dataset. Furthermore, they propose a potential solution that allows the answer distribution of binary questions in the existing abstract scenes dataset to be balanced: for each (image, question) pair, humans were tasked with the creation of a new scene using existing clip-art objects \cite{zitnick2013bringing} that was similar to the existing image, but had a negated answer on the same question, as shown in \figureautorefname{  \ref{fig:zhang2016yin_balancing_procedure}} below.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.35\textwidth]{zhang2016yin_balancing_procedure.png}
    \caption{\citeauthor{zhang2016yin} demonstrate their answer distribution balancing technique. Given the scene on the left and the question ``Is the girl walking the bike'', workers were tasked with creating a scene that differs from the scene on the left but has the opposite answer to the question, as illustrated by the scene and answer on the right.}
    \label{fig:zhang2016yin_balancing_procedure}
\end{figure}

Despite this method's success in reducing the proportion of unbalanced (image, question) pairs by almost 72\%, 20.48\% (image, question) pairs in the balanced dataset did not have a complementary (image, question) pair; 5.93\% because a complementary scene could not be created due to limitations of the abstract scenes clip-art library, and 14.55\% due to disagreement between the answers collected by workers for a given scene. A similar approach was used for non-binary questions in the creation of the VQA 2.0 dataset, as summarised in \tableautorefname{  \ref{tab:dataset_comparison}} above. \citeauthor{hudson2019gqa} approach the task of dataset balancing from a different angle, aiming to mitigate biases in the answer distributions of the GQA dataset whilst maintaining some degree of representation of real-world priors.

Whilst it is certainly important that dataset creators mitigate biases in their work, it is also the responsibility of VQA model authors to ensure that the ways in which image and question data is fed to their models is also able to be justified; many models,  incorporate BiLSTMs \cite{hudson2018compositional} or LSTMs \cite{andreas2016neural} to obtain question-level representations, however by processing the input data in this manner, it becomes difficult to determine the extent to which the reasoning part of the model is exploiting probabilistic priors in the dataset. By embedding and processing the question as a graph, interpretability studies can be performed on the model to ensure the reasoning part of the model performs as intended.

\section{Visual Question Answering Methods}
\label{section:vqa_methods}

It has been observed that the architectures that leverage additional information such as scene graphs rather than just raw image features tend to perform better. This is clearly illustrated by \citeauthor{hudson2019gqa_preprint} in \figureautorefname{ \ref{fig:gqa_input_representation}} below:

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\textwidth]{gqa_input_representation.png}
    \caption{\citeauthor{hudson2019gqa_preprint} illustrate the effect of various forms of input embeddings, noting that accuracy of the trained MAC network \cite{hudson2018compositional} increases drastically with the semantic richness of the embedding.}
    \label{fig:gqa_input_representation}
\end{figure}

\citeauthor{hudson2018compositional} rightly point out that scene graphs cannot always be relied on as a supervisory data source, however with the rise of new scene graph generation models, scene graphs can be generated automatically from images. Since these graphs no longer have to be created through labour-intensive means as seen in the Visual Genome dataset, the use of scene graphs as an image embedding is more feasible, as they can be created quickly for any input image.

\vspace{\baselineskip}

Despite this, very few (if any) models are effectively using similar graph embeddings for question data. By embedding this data similarly to the image, it becomes possible to perform reasoning steps about the image based on the question and vice versa. Commonly used image and quesion embeddings include:

\subsubsection*{Image Representations}
\addcontentsline{toc}{subsubsection}{Image Representations}

\begin{itemize}
    \item CNN features
    \item Image object features and annotations
    \item Scene Graphs
\end{itemize}

\subsubsection*{Question Representations}
\addcontentsline{toc}{subsubsection}{Question Representations}

\begin{itemize}
    \item Traditional word embeddings e.g. word2vec, GloVe
    \item Recurrent neural network embeddings, e.g. LSTM, GRU and their bidirectional counterparts. 
    \item Dependency-based word embeddings \cite{levy2014dependency}
    \item Functional Programs
\end{itemize}

\subsubsection*{Combined Question and Image Representations}
\addcontentsline{toc}{subsubsection}{Combined Question and Image Representations}

\begin{itemize}
    \item Question-Image co-attention \cite{lu2016hierarchical}
    \item Bilinear models e.g. BLOCK Fusion \cite{ben2019block}
\end{itemize}


\subsubsection*{Module Networks}
\addcontentsline{toc}{subsubsection}{Module Networks}

Whilst deep neural networks have proved useful for many classification tasks due to their inherent ability to extract correlations between input and output, they are difficult to interpret and struggle to perform complex reasoning tasks. In order to increase interpretability and coerce deep learning models to perform more structured reasoning steps, \citeauthor{andreas2016neural} proposed the creation of multiple network `modules', each capable of performing a specific reasoning step, similar to each component of the functional programs found in the CLEVR \cite{johnson2017clevr} and GQA \cite{hudson2019gqa} datasets.

\vspace{\baselineskip}

Whilst attaining a state-of-the-art result in 2016 on the VQA 1.0 test dataset \cite{antol2015vqa} of 58.7\%, the per-question-type statistics report only a 2.5\% and 1.4\% performance improvement over a LSTM language-only baseline on binary and numerical questions despite the 16\% improvement on `other'-type questions. This suggests that the success of the model is likely clouded by statistical biases in the dataset, evidenced by the use of a text-only model in the ensemble shown in \figureautorefname{ \ref{fig:andreas2016neural_neural_module_network}}. As discussed by \citeauthor{hudson2018compositional}, the model depends on unreliable parsers and their ``hand-crafted'' design lacks extensibility. Moreover, attention maps on raw CNN features are used as the primary form of image data input. Whilst supplementary information like scene graphs and object annotations were not available at the time of writing, these data formats would allow for additional interactions and data transfer between modules, likely improving the network's ability to perform logical reasoning processes.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\textwidth]{module_network.png}
    \caption{\citeauthor{andreas2016neural} propose the Neural Module Network, in which reasoning operations are strung together based on parser outputs and use combinations and transformations of attention maps on image features to predict a final answer to the question.}
    \label{fig:andreas2016neural_neural_module_network}
\end{figure}

\citeauthor{andreas2016learning} improve upon their earlier work with a more robust method for generating compositional module layouts from questions, combining LSTM embeddings of the question with representations of candidate module layouts to select a module layout using a reinforcement learning approach. This model sees only a slight improvement over their previous iteration, however provides a novel framework for a variety of graph-based reasoning tasks; the scene graphs and functional programs in CLEVR and GQA are strong supervisory training data for such models, and recent scene graph generation techniques \cite{yang2018graph} would prove useful in generalising models to images that have not been previously annotated.

% \subsubsection*{Attention Mechanisms}
% \addcontentsline{toc}{subsubsection}{Attention Mechanisms}

% Attention mechanisms have gained traction in the research community recently, with the rise of non-recurrent models like the Transformer \cite{vaswani2017attention}, and pre-trained models like BERT \cite{devlin2018bert}. Many have realised their potential for multi-modal reasoning tasks, using a variety of attention-based mechanisms to aid reasoning processes.

% \vspace{\baselineskip}

% \citeauthor{hudson2018compositional} leverage the power of self-attention in their Memory-Attention-Composition cell, heavily inspired by recurrent architectures like LSTMs and GRUs. Aiming to shift the focus of VQA models from 
% Bottom-Up and Top-Down:

% MAC: Good, interpretable, but uses BiLSTM for embedding and raw spatial features. What about scene graphs and dependency-based embeddings? (BLOCK-MAC)

% DMCN: 

% DFAF:

% DCAF:

% Hierarchical Question-Image Co-Attention for Visual Question Answering

% - BLOCK:
%   - More efficient than simple bilinear models
%   - Fewer trainable parameters than other fusion techniques, yet outperforms six novel fusion methods.
%   - Not interpretable
  
% - MAC:
%   - Interpretable
%   - Does not use scene graphs or functional programs for supervision, however show a drastic improvement

% \subsubsection*{Multi-Modal Fusion}
% \addcontentsline{toc}{subsubsection}{Multi-Modal Fusion}

% BLOCK Superdiagonal Fusion: Performs decently in terms of VQA metrics, and performs much better than raw bilinear models, however is not interpretable.

% BAN: TODO

\subsection{Graph Methods}

\noindent
\textbf{Aligned Dual Channel Graph Convolutional Networks for VQA} \cite{huang2020aligned}
\label{huang2020aligned}

Current graph-based VQA methods focus only on capturing relationships between objects in images, and process textual information using traditional RNN-based approaches, indicating a need to close the semantic gap between question and image domains. \citeauthor{huang2020aligned} propose graphical representations of question data allow for multi-modal reasoning when reconciled with image objects. The authors of this paper have three main aims:

\begin{enumerate}
    \item To reconcile the objects and their relationships in images with words and their relationships in questions as a way of reasoning on multimodal data.
    \item To develop a method for generating meaningful graphical representations of question data for use in VQA tasks.
    \item To prove that graph-based methods perform on-par or better than existing SOTA VQA approaches.
\end{enumerate}

The methodology behind the model can be summarised as follows:

\begin{itemize}
    \item I-GCN
    \begin{itemize}
        \item Between 10 and 100 object proposals per image, each \(\in \R^{2048}\)
        \item Graphs edges are determined by bounding box intersection-over-union and a linear embedding of object vectors at the edge's endpoints.
    \end{itemize}
    \item Q-GCN
    \begin{itemize}
        \item Questions words represented with 300-dim GloVe embeddings \cite{pennington2014glove}, then encoded using an LSTM.
        \item Graph edge existence is determined using a Stanford dependency parser, and nodes store question word embeddings.
    \end{itemize}
    \item Attention alignment
    \begin{itemize}
        \item Self-attention on question words
        \item Attention on visual features using question attention results as a query.
        \item Linear multimodal fusion between attention outputs \(\tilde{H}_v\) and \(\tilde{H}_q\), with BCE loss computed over the candidate answer vocabulary:
        \[H_r = W_v^\top \tilde{H}_v + W_q^\top \tilde{H}_q\]
        \[softmax(W_e H_r + b_e)\]    
    \end{itemize}
\end{itemize}
Whilst one of the first methods to leverage graph strcutures for question embeddings, the scene graph generation method captures only naive spatial relations, compared to a model like ReGAT which models both explicit and implicit spatial and semantic relationships \cite{li2019relation}. Moreover, dependency-parsed graphs lack spatial and semantic information; they capture structural relationships between words that do not directly translate to spatial/semantic relationships in the joint question-image domain. In addition, the attention alignment module operates on two different data modes, each with their own semantic space; a more successful alignment module would first project data streams onto the same latent space before reasoning about the relationships between image and question objects.

\subsection{Attention Methods}

\subsection{Fusion Methods}

\subsection{Modular Methods}

\section{Model Performance Summary}

\subsection{VQAv2.0}

\begin{table}[htbp]
    \centering
    \begin{tabularx}{\linewidth}{@{\extracolsep{4pt}} l C C C C C C C C @{}}
        \hline
        \multirow{2}{*}{\textbf{Model}} & \multicolumn{4}{c}{Test-dev} & \multicolumn{4}{c}{Test-std} \\
        \cline{2-5}\cline{6-9}
        & All & Yes/No & Num. & Other & All & Yes/No & Num. & Other \\
        \hline
        % Fusion
        BLOCK \cite{ben2019block} & 67.58 & 83.6 & 47.33 & 58.51 & 67.92 & 83.98 & 46.77 & 58.79 \\
        \hline
        % Attention
        % Graph
    \end{tabularx}
    \caption{A comparison of various models for different question types on the VQA v2.0 dataset.}
\end{table}

\subsection{CLEVR}

% \begin{table}[htbp]
%     \centering
%     \begin{tabularx}{\linewidth}{@{\extracolsep{4pt}} l C C C C C @{}}
%         \hline
%         \multirow{2}{*}{\textbf{Model}} & \multicolumn{4}{c}{Test-dev} & Test-std \\
%         \cline{2-5}\cline{6-6}
%         & Y/N & Number & Other & All & All \\
%         \hline
%         MAC \cite{hudson2018compositional} & & & & & \\
%         \hline
%     \end{tabularx}
%     \caption{A comparison of various models on the VQA dataset the accuracy metric.}
% \end{table}

\subsection{GQA}

\begin{table}[htbp]
    \centering
    \begin{tabularx}{\linewidth}{@{\extracolsep{4pt}} l C C C @{}}
        \hline
        \multirow{2}{*}{\textbf{Model}} & \multicolumn{3}{c}{Test} \\
        \cline{2-4}
        & Binary & Open & All \\
        \hline
        % Fusion
        BLOCK \cite{ben2019block} & - & - & - \\
        \hline
    \end{tabularx}
    \caption{A comparison of various models for different question types on the GQA test set using the accuracy metric.}
\end{table}

\begin{table}[htbp]
    \begin{tabularx}{\linewidth}{@{\extracolsep{4pt}} l C C C C C C @{}}
        \hline
        \multirow{2}{*}{\textbf{Model}} & \multicolumn{6}{c}{Test} \\
        \cline{2-7}
        & Distribution & Grounding & Validity & Plausibility & Consistency & Accuracy \\
        \hline
        % Fusion
        BLOCK \cite{ben2019block} & - & - & - & - & - & - \\
        \hline
    \end{tabularx}
    \caption{A comparison of various models on the GQA test set for multiple metrics.}
\end{table}

% Model list 8-12

% Focus
% MAC
% Dual channel GCNs
% ReGAT
% Neural state machine

% Others
% Text GCN

% Methodology
% Graph R-CNN

\subsection*{Interpretability of Multi-modal Reasoning Tasks}
\addcontentsline{toc}{subsection}{Interpretability of Multi-modal Reasoning Tasks}

%%%%%

\subsubsection*{BLOCK: Bilinear Superdiagonal Fusion for Visual Question Answering and Visual Relationship Detection (\cite{ben2019block})}

The authors of this paper focus on the deceptively simple task of fusing multimodal data, specifically visual and textual data for VQA and VRD tasks. They present a block-superdiagonal decomposition for three-dimensional tensors, and demonstrate its effectiveness in constraining the complexity of full bilinear models. Furthermore, they link their work to the famous Tucker and Candecomp/PARAFAC (CP) decompositions, showing that the block decomposition combines both concepts, leveraging the number and size of blocks in the decomposition to find a balance between model complexity and depth of feature combinations. The decomposition is applied to both VQA and VRD tasks, compared against two baseline and six state-of-the-art fusion techniques, outperforming all existing models, despite having fewer trainable parameters than many other models. Given the focus of the paper, the authors unfortunately do not make an effort to investigate the interpretability of the model, leaving room for further experiments using more structured, interpretable input data than the image features and question vectors presented in the paper.

%%%%%

\subsubsection*{Compositional Attention Networks for Machine Reasoning (\cite{hudson2018compositional}), ICLR 2018}

Motivated by the lack of interpretable models capable of performing complex reasoning tasks, \citeauthor{hudson2018compositional} create a robust attention-based model (MAC Network) capable of performing explicit reasoning steps without the external supervision of structured data such as scene graphs or functional programs. They demonstrate the effectiveness of the MAC network by performing experiments using the CLEVR (\cite{johnson2017clevr}) dataset, achieving a new top accuracy of 98.9\%. Additionally, they perform extensive ablations to investigate the effects of different model components on overall performance, and demonstrate its interpretability on various question types by visualising the internal attention weights of the model in terms of the original question and image. In addition to outperforming all existing SOTA models on the full CLEVR dataset, MAC networks also perform surprisingly well on subsets of CLEVR, with an accuracy of up to 35\% higher than other SOTA models on a 10\% subset. Whist the MAC network performs outstandingly on the CLEVR dataset, subsequent studies have shown that its performance decreases on more realistic datasets such as GQA (\cite{hudson2019gqa}), where it achieved only 54.6\%. When incorporating GQA scene graphs into the knowledge base, MAC network performance improved to around 84\%; this suggests that whilst the architecture of the MAC network is strong, providing more structured data to the network allows its cells to perform logical reasoning steps more effectively. In the original paper, question words and context are extracted using a bi-LSTM, however more structured data like semantic graph representations could improve both accuracy and interpretability of the model.

%%%%%

% \subsubsection*{FiLM: Visual Reasoning with a General Conditioning Layer (\cite{DBLP:journals/corr/abs-1709-07871}), AAAI 2018}

% Aim, summary, evaluation and relevance to research problem

%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% Language Priors in VQA Datasets %%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection*{Language Priors and Question Type Distributions in VQA Datasets}
\addcontentsline{toc}{subsection}{Language Priors and Question Type Distributions in VQA Datasets}

%%%%%

\subsubsection*{VQA: Visual Question Answering (\cite{antol2015vqa})}

Given the underwhelming nature of existing visual question answering studies due to small datasets or restricted question or image contexts, the authors of this paper aim to create a diverse dataset that focuses on real-world questions about images. They created two datasets, one containing real-world images pulled from the MSCOCO dataset, and one containing abstract scenes to facilitate pioneering research into VQA tasks. The questions for both datasets were created by humans via an online interface, with a focus on question diversity and image relevance. In addition, ten answers for the questions were collected through a similar interface, and an accuracy metric was devised based on these human responses that accounts for variance in the quality of correct answers. Though this is necessary given the nature of the question generation process, the authors to not highlight why this accuracy metric was chosen over other alternatives; an important discussion given the subjective nature of answer quality. In addition, the question generation process resulted in answer distributions that are skewed towards a particular answer for many question types, encouraging models to rely on language priors to formulate answers without properly incorporating image data into their reasoning processes. However, to the authors' credit, these answer distributions are detailed meticulously in the appendices, but unfortunately these statistics do not find their way into the analysis of models found in many other papers, making it difficult to compare the extent to which different models take advantage of prior probabilities when answering questions.

%%%%%

\subsubsection*{Yin and Yang: Balancing and Answering Binary Visual Questions (\cite{zhang2016yin})}

In this paper, the authors highlight the role of language priors in VQA dataset, providing concrete examples from the VQA 1.0 dataset (\cite{antol2015vqa}) where a simple model can form correct predictions solely based on question words; they show that these priors are present in multiple question types in the VQA 1.0 dataset, pointing out that questions starting with \textit{`What sport is'} can be correctly answered with the word \textit{`tennis'} 41\% of the time, and \textit{`yes'} is over twice as likely to be the answer to binary questions. The authors leverage the flexibility of abstract scenes in an attempt to eliminate language priors by modifying a copy of each image in the VQA abstract scenes dataset such the answer to the original question on the new image is negated. Though novel, this approach does not extend well to real-world image datasets, due to an inability to generate complementary images that are indistinguishable from photographs. Even with the benefits of abstract scenes, the balancing approach is unable to perfectly balance the binary portion of the dataset; for example, a question such as \textit{`Is it raining?'} cannot be negated effectively, since the clip-art object bank does not contain a rain graphic. Moreover, the approach is only applicable to binary questions, which comprise around 41\% of the VQA 1.0 dataset (\cite{antol2015vqa}). To demonstrate the effectiveness of this balancing technique, the authors compare the accuracy of four models when trained on the balanced and unbalanced binary question subset and tested on the unbalanced set of binary questions. They find that the language-only baseline suffers an 18\% decrease when trained on the balanced subset, and more complex models perform between 7-10\% worse. Whilst the paper presents a novel approach to VQA dataset balancing, its inability to completely balance binary questions in real and synthetic datasets alike makes it ineffective as a method for standardising VQA datasets.

%%%%%

% \subsubsection*{Making the V in VQA Matter: Elevating the Role of Image Understanding in Visual Question Answering (\cite{DBLP:journals/corr/GoyalKSBP16})}

% Aim, summary, evaluation and relevance to research problem

%%%%%

\subsubsection*{GQA: A New Dataset for Real-World Visual Reasoning and Compositional Question Answering (\cite{hudson2019gqa}), CVPR 2019}

Given the prevalence of statistical biases present in other VQA datasets, \citeauthor{hudson2019gqa} provide a VQA dataset with compositional questions grounded on real-world images, balancing answer distributions during dataset creation. In addition, image scene graphs and functional programs are used as tools to create the dataset, as well as provide additional supervision options to potential models, and form a basis for new evaluation metics: distribution, grounding, validity, plausibility and consistency. The effectiveness of these metrics in evaluating VQA models is demonstrated by a set of baseline tests on probabilistic prior, vision-only (CNN), language-only (LSTM), naive language and vision (CNN+LSTM) and state-of-the-art attention base models (BottomUp and MAC). The balancing procedure used aims to find a balance between uniformly distributing answers for certain question types and maintaining relevant real-world priors by setting upper and lower bounds on the occurrence ratio of different answers for a given question type. Whilst this approach enables a tunable degree of distribution-balancing, the authors do not discuss how they determined the best parameters to use in this process, stating only that ``the entropy of the answer distribution increased by 72\%'' after balancing. One major improvement that \citeauthor{hudson2019gqa} make over the VQA datasets is that they leverage structured data in their new metrics, shifting focus from debates about what makes an `accurate' answer to more human-centered measures like how plausible, valid and consistent a model's answers are. Whilst these metrics are certainly useful, they are not usable in other datasets due to lack of information. This has made the comparison of SOTA models more complex, highlighting a need for frequent summaries of model performance on various relevant datasets. This need is further substantiated by the absence of questions requiring external or common-sense knowledge, such as \textit{`Why is it raining?'} or \textit{`What sound does the animal make?'}, as found in the original VQA datasets.