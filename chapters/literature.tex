\chapter{Literature Review}

\section{Visual Question Answering Datasets}

\begin{center}
\begin{xltabular}{\textwidth}{LLLLLLLLL}
  \toprule
  \textbf{Dataset}
  & \textbf{Year}
  & \textbf{Image Count}
  & \textbf{Question Count}
  & \textbf{Image Source}
  & \textbf{Question Source}
  & \textbf{Answer Type}
  % & \textbf{Answer Vocabulary}
  & \textbf{Additional Data}
  & \textbf{Evaluation Metrics}\\
  \midrule
  \textbf{DAQUAR} \cite{malinowski2014multiworld}
  & 2014
  & 1K  % 1449 images
  & 12K  % 12,468 questions
  & NYU-Depth V2 \cite{silberman2012indoor}
  & Both
  & Multi-label  % 37 classes for Reduced DAQUAR
  % & 894 classes
  & -
  & Accuracy, WUPS \\
  \textbf{Visual Madlibs} \cite{yu2015visual}
  & 2015
  & 10K  % 10,738 images  % Filtered hard task?
  & 360K  % 360,001 descriptions
  & COCO \cite{lin2014microsoft}
  & Human
  & Fill in the blank open-ended \& multi-choice
  % & -
  & -
  & Accuracy, BLEU \\
  \textbf{COCO-QA}
  & -
  & -
  & -
  & COCO
  & -
  & -
  % & -
  & -
  & Accuracy, BLEU \\
  \textbf{VQAv1} \cite{antol2015vqa}
  & 2015
  & 204K  % 204,721 images
  & 614K  % 614,163 questions % 6,141,630 ground truth answers % 1,842,489 plausible answers
  & COCO
  & Human
  & Open-ended, Multi-choice
  % & {\color{red}- words}, 18 classes
  & COCO image captions
  & Accuracy\footnotemark \\
  Abstract Scenes
  & 2015
  & 50K  % 50,000 abstract scenes
  & 150K  % 150,000 questions % 1,500,000 ground truth answers % 450,000 plausible answers % 250,000 captions
  & Clip art, 2D
  & Human
  & Open-ended, Multi-choice
  % & {\color{red}- words}, 18 classes
  & Image captions
  & Accuracy\footnotemark[\value{footnote}] \\
  Changing Priors (CP) \cite{agrawal2018dont}
  & 2018
  & {\color{red}\(\approx\)204K}  % Exact unkown as calculation was based on sum of train and test which are not necessarily mutually exclusive on images.
  & {\color{red}\(\approx\)370K}  % Exact unkown, ~ 3.8M answers
  & COCO
  & Human
  & Open-ended
  % & {\color{red}- words, - classes}
  & See VQAv1
  & Accuracy\footnotemark[\value{footnote}] \\
  Compositional VQA (C-VQA) \cite{agrawal2017cvqa}
  & 2017
  & 204K  % Some images repeated in train and test
  & 369K  % 369,861 questions, 3,698,610 answers
  & COCO
  & Human
  & Open-ended
  % & -
  & See VQAv1
  & Accuracy\footnotemark[\value{footnote}] \\
  \textbf{VQAv2} \cite{goyal2017making}
  & 2017
  & 204K  % 204,721 images
  & 1.1M  % 1,105,904 questions % 11,059,040 ground truth answers
  & COCO
  & Human
  & Open-ended
  % & {\color{red}- words}
  & COCO image captions, Complementary image pairs
  & Accuracy\footnotemark[\value{footnote}] \\
  Balanced Binary Abstract Scenes \cite{zhang2016yin}
  & 2016-17
  & 31K % 31,325 abstract scenes
  & 33K % 33,383 questions % 333,830 ground truth answers
  & Clip art, 2D
  & Human
  & Multiple choice
  % & `yes', `no'
  & Image captions
  & Accuracy\footnotemark[\value{footnote}] \\
  Changing Priors (CP) \cite{agrawal2018dont}
  & 2018
  & {\color{red}\(\approx\)219K}  % Exact unkown as calculation was based on sum of train and test which are not necessarily mutually exclusive on images.
  & {\color{red}\(\approx\)658K}  % Exact unkown, ~ 6.6M Answers
  & COCO
  & Human
  & Open-ended
  % & {\color{red}- words}
  & See VQAv2
  & Accuracy\footnotemark[\value{footnote}] \\
  \textbf{Visual Genome} \cite{krishna2017visual}
  & 2016
  & 108K  % 108,077 images
  & 1.7M  % 1,773,258 questions and answers
  & COCO, YFCC100M \cite{thomee2016yfcc100m}
  & Human
  & Open-ended
  % & -
  & COCO annotations, Region descriptions, Scene graphs
  & Accuracy \\
  \textbf{Visual7W} \cite{zhu2016visual7w}
  & 2016
  & 47K % 47,300
  & 327K % 327,939
  & COCO
  & Human
  & -
  % & -
  & - % MC?
  & - \\
  \textbf{TDIUC} \cite{kafle2017analysis}
  & 2017
  & 167K % 167,437 images
  & 1.6M % 1,654,167 questions
  & COCO, Visual Genome
  & Both
  & Open-ended
  % & -
  & -
  & Per-question-type accuracy, regular \& normalised arithmetic \& harmonic mean accuracy\\
  \textbf{CLEVR} \cite{johnson2017clevr}
  & 2017
  & 100K  % 100,000 images
  & 999K  % 999,968 questions
  & Computer-generated, 3D
  & Generated
  & Open-ended
  % & -
  & Functional programs, Scene graphs
  & Accuracy \\
  CoGenT-A \& B
  & 2017
  & 100K
  & 999K
  & Computer-generated, 3D 
  & Generated
  & Open-ended
  % & -
  & Functional programs, Scene graphs
  & Accuracy \\
  Humans
  & 2017
  & -
  & 32K  % 32,164
  & CLEVR
  & Human
  & Open-ended
  % & -
  & See CLEVR
  & Accuracy \\
  \textbf{GQA} \cite{hudson2019gqa}
  & 2019
  & 113K  % 113,018 images
  & 22.6M  % 22,669,678 questions
  & -
  & Both
  & Open-ended
  % & 1878 unique answers  % 1878 is for all splits. train/val/test-dev answers are 1-3 words.
  & Scene graphs, Functional programs, Full-sentence answers
  & Accuracy, Consistency, Validity, Plausibility, Distribution, Grounding \\
  %  & - & - & - \\
  % \textbf{V7W} \cite{zhu2016visual7w} & - & 47K & 0.32M & Multiple choice \\
  % \textbf{TDIUC} \cite{kafle2017analysis} & - & 167K & 1.6M & Open-ended \\
  \bottomrule
  \caption{A comparison of relevant features of the most popular VQA datasets. Dataset variations are listed in regular font below their bolded counterparts.}
  \label{tab:dataset_comparison}
\end{xltabular}
\end{center}

\footnotetext{For open-ended answers, an answer is considered `correct' if it matches at least three of the ten human-provided answers. For multiple choice answers, a traditional accuracy metric is used.}


\section{Question Embedding in Visual Question Answering}

\section{Image Embedding in Visual Question Answering}

\section{Multi-modal Fusion in Visual Question Answering}