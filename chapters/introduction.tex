\chapter{Introduction}
\label{chapter:introduction}

We live in an exciting technological era. In the last decade, we have witnessed the emergence of smart-home devices that harness the power of complex natural language understanding, text-to-speech and speech-to-text models to assist millions of people every day. The collaboration of hardware and software engineers have fueled the rapid evolution of the deep learning field, leading us into a world where semi-autonomous vehicles are becoming commonplace and face detection models are being used to unlock mobile devices. Given the wide adoption of deep learning (DL) techniques in industry for natural language processing (NLP) and computer vision (CV) tasks, we have seen a recent shift in the research community towards audio-visual and visio-linguistic tasks that require the learning of complex interactions between multiple input data modes. As a research community, we are excited by the technical challenge that these new tasks present, in addition to the extensive practical applications of multimodal reasoning tasks; for example, well-designed image captioning and visual question answering models could aid the visually impaired in consuming and understanding visual information in a natural manner when combined with existing speech-to-text and text-to-speech systems. Other multimodal tasks like text-to-image generation could be leveraged by law enforcement teams to build realistic composites of people or places relevant to an investigation, or by creatives as an external source of visual inspiration.

% widely-adopted text-to-speech models could be made more amiable with the aid of realistic speech synthesis models.
% Information retrieval systems would also reap the benefits of image captioning research contributions, as dense amounts of image metadata could be generated to improve search results.

% #TODO revisit wording of multimodal task applications.

% #TODO Consider a more in-depth discussion of colourblindness as mentioned in the presentation?

% More formally, solving the VQA problem requires learning a function \(h_S(q, i) \rightarrow a\) where \(q \in Q\), \(i \in I\), \(a \in A\) for a set of questions, images and answers \(Q\), \(I\) and \(A\) respectively.

In this dissertation, I focus solely on visual question answering (VQA). More specifically, I delve into how we can leverage graphical representations of both visual and textual data to aid reasoning models in their decision-making processes. At its core, the VQA problem takes two inputs, an image and a question pertaining to one or more objects, relationships and/or concepts presented in the image. Given these inputs, we aim to answer the question, as illustrated in Figure \ref{fig:dataset_samples}. As humans, we implicitly solve the VQA problem every day when making decisions grounded on visual inputs. When crossing the road, we ask the implicit question \textit{`Is it safe to cross the road?'}, formulate an answer based on visual signals, and then act upon our decision. The holy grail of VQA is to be able to perform such decision-making for all feasible combinations of visual inputs and questions. Naturally, the true distribution of these input combinations is unknown, motivating the first major hurdle in VQA research: \textit{How do we design a dataset that effectively captures subtle relationships between questions and images as presented in real-world situations where VQA would prove useful?} 

As evident in \figureautorefname{ \ref{fig:dataset_samples}}, this question is still open to interpretation; the VQAv2 dataset contains real-world images and free-form questions often requiring conceptual reasoning, whilst CLEVR leverages generated images alongside questions requiring more compositional reasoning, targeting logical reasoning operations like counting and boolean arithmetic. The GQA dataset combines the approaches of VQAv2 and CLEVR datasets by combining the complexity of real-world images with the difficulty of questions requiring multiple discrete reasoning steps. I explore the methods used to create these three datasets in more detail in Section \ref{section:vqa_datasets}.

\begin{figure}
  \begin{subfigure}[t]{0.3\textwidth}
    \centering
    \includegraphics[width=\textwidth]{vqa_sample.jpg} 
    \caption{Is there something to cut the vegetables with? \textit{no}}
    \label{fig:vqa_sample}
  \end{subfigure}
  \hfill
  \begin{subfigure}[t]{0.3\textwidth}
    \centering
    \includegraphics[width=\textwidth]{clevr_sample.jpg} 
    \caption{How many objects are either small cylinders or red things? \textit{5}}
    \label{fig:clevr_sample}
  \end{subfigure}
  \hfill
  \begin{subfigure}[t]{0.3\textwidth}
    \centering
    \includegraphics[width=\textwidth]{gqa_sample.jpg} 
    \caption{What kind of furniture is to the right of the chair? \textit{sofa}}
    \label{fig:gqa_sample}
  \end{subfigure}
  \caption[Example instances from the VQAv2, CLEVR and GQA datasets.]{Example image, question and answer triples from the VQAv2 \cite{goyal2017making}, CLEVR \cite{johnson2017clevr} and GQA \cite{hudson2019gqa} datasets, figures \ref{fig:vqa_sample}, \ref{fig:clevr_sample} and \ref{fig:gqa_sample} respectively.}
  \label{fig:dataset_samples}
\end{figure}

Assuming we do have an ideal dataset that effectively models real-world VQA problems, we need some way of combining both visual and textual data in a way that enables a model to perform the complex reasoning required for answer formulation.

A wide variety of VQA models have been proposed for this task over the last few years, with each approaching the task from a different angle. Early VQA methods relied on convolutional neural network (CNN) features such as those obtained from ResNet \cite{he2016deep} or object-based features such as those obtained from Faster R-CNN \cite{ren2016faster}. In addition, they leveraged recurrent models such as Long Short Term Memory (LSTM) \cite{hochreiter1997long} networks or Gated Recurrent Units (GRU) \cite{cho2014learning} as question embeddings. Many researchers proposed different ways to fuse these image and question features, with the three main categories being attention-based methods, fusion-based methods and graph-based methods.

Attention-based methods started out as simple pointwise multiplications \cite{antol2015vqa}, or question-guided image attention mechanisms \cite{anderson2018bottom}, before progressing to self-attention \cite{hudson2018compositional}, transformer-style attention \cite{vaswani2017attention}, co-attention \cite{lu2016hierarchical, yu2019deep} and bilinear attention \cite{kim2018bilinear} mechanisms.

Where attention mechanisms like co-attention and bilinear attention aim to highlight which parts of the image are important to the question and vice-versa, fusion methods aim to project the question and image domains onto a single semantic space. Simple fusion methods such as concatenation appear everywhere in the VQA field, however dedicated fusion methods like MUTAN \cite{ben2017mutan} and BLOCK \cite{ben2019block} aim to approximate more effective but expensive bilinear fusion methods by using various tensor decompositions.

With the inclusion of scene graphs in various VQA datasets, VQA research has more recently shifted more towards graphical methods, which leverage Graph Convolutional Networks (GCN) \cite{kipf2016semi} or Graph Attention Networks (GAT) \cite{velivckovic2017graph} for capturing contextual relationships between image object, attribute and relationship information. Methods like Relation Aware Graph Attention networks (ReGAT) \cite{li2019relation} and Dual-Channel GCN (DC-GCN) \cite{huang2020aligned} have proven effective on popular datasets like VQAv2 \cite{goyal2017making} and VQA-CP \cite{agrawal2018dont}. With an increase in graph-based VQA models has come an increase in scene-graph generation methods, which focus on generating scene graphs from images. However, as mentioned briefly in the abstract, many graph-based VQA models use arbitrary scene graph generation methods in addition to experimenting with new question answering architectures, making it difficult to determine whether performance increases are due to improved scene graph generation methods or improved visual reasoning models.

For this reason, I focus solely on the the visual question answering task in this dissertation, in an effort to demonstrate the effectiveness of graph-based methods like GATs for visual reasoning. To build my VQA model, I rely heavily on datasets like GQA \cite{hudson2018compositional}, which come with pre-annotated scene graphs, and draw inspiration from existing attention-based and graph-based architectures. Consequently, I devote a large portion of \chapterautorefname{ \ref{chapter:literature}} to exploring various VQA datasets and prevalent graph-based VQA methods to better understand why scene graphs in combination with graph-based models perform so well for VQA tasks.

\section{Contributions}

My primary contributions to the VQA field are three-fold:

\begin{enumerate}
    \item I propose a scene graph transformation method that allows object-object relationships to be embedded as nodes alongside object and attribute embeddings. This allows node-based graph convolutions to operate on the modified scene graph, capturing contextual object, attribute and relationship information. Moreover, I provide a theoretical upper bound on the time complexity of applying GAT convolutions on the modified scene graph; given an original scene graph \(r\) containing \(r_o\) objects, \(r_a\) attributes and \(r_r\) relations, we define \(\rho = \frac{r_r}{r_o}\), and \(F\) to be the input dimension of a given GAT layer. Given that my graph transformation method yields a new graph \(\mathcal{G}_r\), I prove that performing a GAT convolution on \(\mathcal{G}_r\) is in the order of \(\min\{\rho, F\}\) times slower than performing a GAT convolution on \(r\). For the GQA, \(\rho \approx 3\), meaning that performing GAT convolutions on \(\mathcal{G}_r\) is both feasible and effective, as it allows traditional node-based convolutions to operate on what would usually be an edge embedding.
    \item I present a new, simple and effective  VQA model that combines my novel graph transformation method, a graph attention network, a question processing module such as a BiLSTM, and a compositional attention network. When evaluated on the GQA dataset, it outperforms existing scene-graph oriented models on the top-1 accuracy measure by over 9\%, as well as current state-of-the-art image-based ensemble methods\footnote{According to the GQA leaderboard \cite{gqachallenge}} by over 14\%, and outperforms even human accuracy by 1\%, binary question type accuracy by almost 0.4\%, and open-type accuracy by 1.7\%.
    \item In the spirit of reproducibility, I provide the full source code for my primary model architecture and various ablations online at \url{https://github.com/alexmirrington/gat-vqa}.
\end{enumerate}

\section{Thesis Structure}

In the introduction to this dissertation, I provide an overview of the VQA problem and its uses, before introducing a variety of VQA datasets and methods to provide the reader with an understanding of the breadth of the field.

In \chapterautorefname{ \ref{chapter:literature}}, I delve further into the design decisions behind various VQA datasets, focusing on how these datasets are created at such a large scale whilst still remaining relatively unbiased in their question and answer distributions, as well as how different authors approach the concept of scene graphs for VQA. Next, I expand on the VQA model architectures mentioned in the discussion that inspire or form part of my model architecture, before finalising the literature review with a summary of various evaluation metrics for different VQA tasks.

In \chapterautorefname{ \ref{chapter:methodology}}, I introduce my model architecture, describing its four main components: the question embedding and processing module, the scene graph embedding and processing module, the reasoning module, and the output module. I also analyse the complexity of my scene graph embedding process in this chapter, and summarise the primary motivations behind the inclusion of each model component.

In \chapterautorefname{ \ref{chapter:evaluation}}, I introduce the GQA dataset in more detail, before providing a brief overview of the baseline VQA methods that I compare my model's performance against. Finally, I provide a detailed list of my model's parameter configuration, including its dataset vocabulary sizes, training schedule and optimisation method.

In \chapterautorefname{ \ref{chapter:results}}, I report and comment on the performance of my model compared to baseline models for a variety of metrics. Moreover, I provide details about the ablation studies I performed, presenting empirical evidence to support my model architecture design decisions. Finally, I provide a brief overview of the parameter optimisation process I used to obtain the set of final model parameters introduced in \chapterautorefname{ \ref{chapter:evaluation}}.

In \chapterautorefname{ \ref{chapter:discussion}}, I analyse how my model arrives at an answer given a scene graph and a question, delving into the attention maps used to support the reasoning processes of the scene graph processing module and reasoning modules.

Finally, in \chapterautorefname{ \ref{chapter:conclusion}}, I provide closing remarks on the primary observations of each chapter in this dissertation, and briefly explore potential directions for future research.
