\chapter{Introduction}


We live in an exciting technological era. In the last decade, we have witnessed the emergence of smart-home devices that harness the power of complex natural language understanding, text-to-speech and speech-to-text models to assist millions of people every day. The collaboration of hardware and software engineers have fueled the rapid evolution of the deep learning field, leading us into a world where semi-autonomous vehicles are becoming commonplace and face detection models are being used to unlock mobile devices. Given the wide adoption of deep learning (DL) techniques in industry for natural language processing (NLP) and computer vision (CV) tasks, we have seen a recent shift in the research community towards audio-visual and visio-linguistic tasks that require the learning of complex interactions between multiple input data modes. As a research community, we are excited by the technical challenge that these new tasks present, in addition to the extensive practical applications of multimodal reasoning tasks; for example, well-designed image captioning and visual question answering models could aid the visually impaired in consuming and understanding visual information in a natural manner when combined with existing speech-to-text and text-to-speech systems. Other multimodal tasks like text-to-image generation could be leveraged by law enforcement teams to build realistic composites of people or places relevant to an investigation, or by creatives as an external source of visual inspiration.

% widely-adopted text-to-speech models could be made more amiable with the aid of realistic speech synthesis models.
% Information retrieval systems would also reap the benefits of image captioning research contributions, as dense amounts of image metadata could be generated to improve search results.

% #TODO Consider a more in-depth discussion of colourblindness as mentioned in the presentation?

In this dissertation, I focus solely on visual question answering (VQA), a multimodal task that requires a model to can be formally modelled as a function \(f(Q, I) \rightarrow A\)

% Given the research challenge and practical applications of the visual question answering (VQA) task, I believe it is a great candidate for my own research. Over the last few years, the deep learning and natural language processing communities have realised a variety of VQA models, and new VQA datasets have been created over time to accommodate for both the increasing capabilities VQA models, and the shortcomings of existing VQA datasets that models have learned to exploit. In this literature review, I delve into both the methods used to generate some of the most popular VQA datasets as well as the image and question representations used by numerous state-of-the-art models, with the aim of emphasising the importance of graph structures for visual reasoning tasks; the more recent, challenging VQA datasets such as CLEVR \cite{johnson2017clevr} and GQA \cite{hudson2019gqa} leverage graph structures grounded in images for the generation of questions, suggesting that models could leverage graph structures in the question answering process. Many state-of-the-art models \cite{hudson2018compositional} still rely on CNN features and traditional word embeddings as data inputs, or at best, leverage image scene graphs to aid the learning process. I propose that by mapping both images and questions to interpretable graph structures that adequately capture the relationships between objects and concepts present in the input data, VQA models will be able to focus on the reasoning steps required to solve the problem rather than trying to identify object relationships and learn valid reasoning processes simultaneously.

\section{Contributions}

Summary of main contributions to the field.

\section{Outline}

Overall thesis outline.