\chapter{Methodology}
\label{chapter:methodology}

\section{Architecture Overview}
\label{section:architecture_overview}

In this section, I propose a new model for single-concept open-ended visual question answering tasks. The proposed model comprises six main components, as shown in \figureautorefname{ \ref{fig:model_overview}} and outlined briefly below:

\begin{figure}[htbp]
    \centering
    \includegraphics[width=\textwidth]{model_overview.png}
    \caption{A high-level overview of the proposed visual-question-answering model.}
    \label{fig:model_overview}
\end{figure}

\textbf{Question embedding:} In classic NLP fashion, the question embedding component is responsible for the preprocessing and vectorisation of raw question data for use by the rest of the model.

\textbf{Question module:} The question module has two main roles: the first is to learn a sequence of contextual words {\color{red}(TODO: Insert nomenclature)} that capture the relevant features of each question word in the context of the question as a whole. Secondly, the question module learns a question-level embedding that contains information about the question as a whole.

\textbf{Scene graph embedding:} The scene graph embedding model is responsible for the preprocessing and vectorisation of scene graph data. In this dissertation, I use ground-truth scene graphs from the GQA dataset to evaluate the effectiveness of various scene graph modules under perfect-sight conditions. This component can be readily extended to utilise the output of existing scene graph generation models \cite{yang2018graph, li2019relation}, yielding a true end-to-end VQA model.

\textbf{Scene graph module:} The primary goal of the scene graph module is to learn a dense representation of scene graph features that can be used as a knowledge-base for the reasoning module to draw information from when answering questions.

\textbf{Reasoning module:} Given the output features of the scene graph and question modules, the reasoning module is in charge of determining which visual features are relevant to the question and vice-versa. The reasoning capabilities of many existing VQA models could be leveraged here; I justify my reasoning module of choice and describe its implementation in detail in \sectionautorefname{ \ref{section:reasoning_module}}.

\textbf{Output module:} The output module is the simplest part of the architecture, and is responsible for transforming the output of the reasoning module into a concrete answer to the input question. I delve into the design decisions behind the output module and how it might be modified for other VQA tasks in \sectionautorefname{ \ref{section:output_module}}.

\section{Question Embedding}
\label{section:question_embedding}

The question embedding component of the architecture is responsible for two major tasks: preprocessing and vectorisation. The preprocessing step is performed prior to the training of the model, where vectorisation of preprocessed question words occurs at runtime. 

I use a neural preprocessing pipeline for each raw question string in the dataset, comprising tokenisation, part-of-speech (POS) tagging, lemmatisation and dependency parsing steps.\footnote{For this architecture, only the tokenisation step is required since the question module uses word embeddings but not syntactic dependency information. Graph-based question modules use syntactic dependencies for graph construction, as described in \sectionautorefname{ \ref{sec:ablation_studies}}.} For reproducibility, I use the Stanza NLP package \cite{qi2020stanza} for all steps of the pipeline.

After tokenisation, each question token is converted to an index to enable vector embedding lookups during training. A mapping of all question tokens to their respective indices is kept, and shared between the training and validation sets. Reference answers are converted to indices in a similar fashion, however they are not tokenised first. Multi-word answers like \textit{parking meter} or \textit{cutting board} are treated as a single concept, identically to single-word answers like \textit{bed}. This ensures  multi-word answers can be predicted just as easily as single-word answers and allows the output module to treat the question-answering task as a multi-class classification problem across the set of all candidate answers.

At runtime, each question token is converted to a \(d = 300\) dimensional GloVe vector \cite{pennington2014glove}, then token embeddings for a given question are stacked together, yielding a matrix of question features \(H_q \in \R^{l_q \times d}\) where \(l_q\) is the number of tokens in the question. For a batch of questions \(Q = \{q_1, ..., q_n\}\) the corresponding features \(\{H_{q_1}, ..., H_{q_n}\}\) are padded with zeros along the first dimension and then stacked to yield a batch of question features \(H_Q \in \R^{n \times l \times d}\), where \(l = \max_{i} l_{q_i}\). \(H_Q\) is the output of the question embedding module and the input to the question module.

\section{Question Module}
\label{section:question_module}

BiLSTM \cite{hochreiter1997long}

I describe variations of the question module in \sectionautorefname{ \ref{sec:ablation_studies}}.


\section{Scene Graph Embedding}
\label{section:scene_graph_embedding}

Both the GQA and CLEVR datasets contain ground-truth scene graph information for each image in the training and validation sets. Although different datasets store scene graph information differently, the three basic components of a scene graph are objects, relationships and attributes. For example, the CLEVR dataset contains three object types (cube, sphere, cylinder), twelve attribute types covering size (small, large), material (metal, rubber) and colour (red, green, blue, yellow, purple, cyan, brown, grey), and four spatial relationships (left, right, in front, behind).
 
The primary goal of the scene graph embedding module is to build a directed graph \(\mathcal{G}_v = (\mathcal{V}_v, \mathcal{E}_v)\) and an associated node feature matrix \(H_v \in \R^{|\mathcal{V}| \times d}\) from a visual input \(v\). In this dissertation, I intentionally use gold-standard scene graphs as a visual input to evaluate the effectiveness of different scene graph modules, as discussed in \sectionautorefname{ \ref{sec:ablation_studies}}. Hence, for the remainder of this section, I will refer to a scene graph for an individual dataset sample as \(v = (v_o, v_a, v_r)\) where \(v_o, v_a\) and \(v_r\) are sets of objects, attributes and relationships respectively.

The preprocessing of objects, relations and attributes is similar to the way answers were processed in the question embedding embedding step: all objects, relations and attributes are converted to indices, keeping a separate map of tokens to indices for each. This ensures that words like \textit{glass} are distinguished based on whether they are used as an object or an attribute, allowing the scene graph module to learn different embeddings for each based on the contexts in which they are used.

At runtime, each object, relation and attribute index is associated with a \(d = 300\) dimensional GloVe vector, then all vectors are stacked together to produce an embedding matrix of scene graph node features \(H_V\) spanning the object, relation and attribute vocabulary of the dataset \(S\), \textit{i.e.} \(H_V \in \R^{(|S_o| + |S_r| + |S_a|) \times d}\) where \(S_o = \{v_o \mid ((q, v), y) \in S\}\) is the set of unique objects in \(S\), \(S_r = \{v_r \mid ((q, v), y) \in S\}\) and \(S_a = \{v_a \mid ((q, v), y) \in S\}\). For the GQA dataset, there are a total of \(|S_o| = 1703\) unique objects, \(|S_r| = 310\) unique relations, and \(|S_a| = 617\) unique attributes across the train and validation sets. These global node features are shared by all scene graphs, and fine-tuned via backpropagation  during training.

{\color{blue} Pick up from here}

 As each batch of scene graphs is loaded into memory, we select the relevant node embeddings according \(\)


y, \(\mathcal{G}_v\) and \(H_v\) are created according to \algorithmcfname{ \ref{algorithm:scene_graph_construction}}:

% for obj_dict in objects:
%     labels.append([])
%     boxes.append([])
%     attrs.append([])
%     coos.append(([], []))
%     relations.append([])
%     obj_key_to_idx = {key: idx for idx, key in enumerate(obj_dict.keys())}
%     for obj_key, obj_data in obj_dict.items():
%         name = obj_data["name"]
%         box = (
%             obj_data["x"],
%             obj_data["y"],
%             obj_data["x"] + obj_data["w"],
%             obj_data["y"] + obj_data["h"],
%         )
%         labels[-1].append(name)
%         if name not in self._object_to_index:
%             self._object_to_index[name] = len(self._object_to_index)
%         boxes[-1].append(box)
%         attrs[-1].append(obj_data["attributes"])
%         for attr in obj_data["attributes"]:
%             if attr not in self._attr_to_index:
%                 self._attr_to_index[attr] = len(self._attr_to_index)
%         # Populate relation indices
%         for relation in obj_data["relations"]:
%             coos[-1][0].append(obj_key_to_idx[obj_key])
%             coos[-1][1].append(obj_key_to_idx[relation["object"]])
%             relations[-1].append(relation["name"])
%             if relation["name"] not in self._rel_to_index:
%                 self._rel_to_index[relation["name"]] = len(self._rel_to_index)
% return boxes, labels, attrs, coos, relations

\begin{algorithm}
    \Function{\(v_o, v_a, v_r\)}{
        \(\mathcal{V}_v, \mathcal{E}_v\)
        % \(s_c \leftarrow 0\)\\
        % \ForEach{\(\{q_i \in Q \mid E_{q_i} \neq \emptyset\}\)}{
        %     \(s_{q_i} \leftarrow 0\)\\
        %     \ForEach{
        %         \(q_e \in E_{q_i}\)
        %     }{
        %         \lIf{
        %             \(q_e \in Q\)
        %         }{
        %             \(s_{q_i} \leftarrow s_{q_i} + 1\)
        %         }
        %     }
        %     \(s_c \leftarrow s_c + \frac{s_{q_i}}{|E_{q_i}|}\)
        % }
        % \Return \(\frac{s_c}{|\{q_i \in Q \mid E_{q_i} \neq \emptyset\}|}\)
    }
    \caption[Scene Graph Construction Algorithm]{Scene Graph Construction Algorithm}
    \label{algorithm:scene_graph_construction}
\end{algorithm}

As illustrated in \figureautorefname{ \ref{fig:model_overview}}, and described in \algorithmcfname{ \ref{algorithm:scene_graph_construction}},  \(\mathcal{V}_V\) contains a node for every object, relation and attribute in the scene graph. Given we have \(n_o\) objects, \(n_a\) attributes and \(n_r\) relations, we see that scene graph construction method has the obvious downfall of containing \(O(n_o + n_a + n_r) = O(n_o^2)\) nodes. Nonetheless, constructing the scene graph in this manner allows the direct application of sparse node-based graph convolutional layers in the scene graph module.
{\color{red} TODO: Further work on edge convolutions?}

% CLEVR scene graph example:

% \begin{verbatim}
%     {"image_index": 0, "objects": [{"color": "blue", "size": "large", "rotation": 269.8517172617167, "shape": "cube", "3d_coords": [-1.3705521821975708, 2.0794010162353516, 0.699999988079071], "material": "rubber", "pixel_coords": [269, 88, 12.661545753479004]}, {"color": "green", "size": "large", "rotation": 292.2219458666971, "shape": "cylinder", "3d_coords": [-2.9289753437042236, -1.7488206624984741, 0.699999988079071], "material": "metal", "pixel_coords": [93, 108, 11.522202491760254]}, {"color": "cyan", "size": "small", "rotation": 25.545135239473026, "shape": "cube", "3d_coords": [1.5515961647033691, 0.6776641607284546, 0.3499999940395355], "material": "rubber", "pixel_coords": [319, 162, 10.045343399047852]}, {"color": "brown", "size": "large", "rotation": 327.3489188814305, "shape": "cylinder", "3d_coords": [-0.25301405787467957, -2.3089325428009033, 0.699999988079071], "material": "metal", "pixel_coords": [132, 159, 9.392304420471191]}, {"color": "gray", "size": "small", "rotation": 6.325183772442613, "shape": "cube", "3d_coords": [1.018894076347351, -1.93693208694458, 0.3499999940395355], "material": "rubber", "pixel_coords": [192, 197, 8.907766342163086]}, {"color": "brown", "size": "large", "rotation": 25.96049348342493, "shape": "sphere", "3d_coords": [0.43993687629699707, 2.9987525939941406, 0.699999988079071], "material": "metal", "pixel_coords": [353, 100, 11.964213371276855]}], "relationships": {"right": [[2, 5], [0, 2, 3, 4, 5], [5], [0, 2, 4, 5], [0, 2, 5], []], "behind": [[], [0, 5], [0, 1, 5], [0, 1, 2, 5], [0, 1, 2, 3, 5], [0]], "front": [[1, 2, 3, 4, 5], [2, 3, 4], [3, 4], [4], [], [1, 2, 3, 4]], "left": [[1, 3, 4], [], [0, 1, 3, 4], [1], [1, 3], [0, 1, 2, 3, 4]]}, "image_filename": "CLEVR_train_000000.png", "split": "train", "directions": {"right": [0.6563112735748291, 0.7544902563095093, -0.0], "behind": [-0.754490315914154, 0.6563112735748291, 0.0], "above": [0.0, 0.0, 1.0], "below": [-0.0, -0.0, -1.0], "left": [-0.6563112735748291, -0.7544902563095093, 0.0], "front": [0.754490315914154, -0.6563112735748291, -0.0]}}
% \end{verbatim}

% GQA scene graph example:

% \begin{verbatim}
% "2386621": {"width": 500, "objects": {"681267": {"name": "banana", "h": 34, "relations": [{"object": "681262", "name": "to the left of"}], "w": 64, "attributes": ["small", "yellow"], "y": 55, "x": 248}, "681265": {"name": "spots", "h": 16, "relations": [], "w": 26, "attributes": [], "y": 92, "x": 245}, "681264": {"name": "bananas", "h": 50, "relations": [{"object": "681259", "name": "to the left of"}], "w": 49, "attributes": ["small", "yellow"], "y": 32, "x": 268}, "681263": {"name": "picnic", "h": 374, "relations": [], "w": 499, "attributes": ["delicious"], "y": 0, "x": 0}, "681262": {"name": "straw", "h": 95, "relations": [{"object": "681268", "name": "to the right of"}, {"object": "681267", "name": "to the right of"}, {"object": "681253", "name": "to the right of"}], "w": 15, "attributes": ["white", "plastic"], "y": 55, "x": 402}, "681261": {"name": "meat", "h": 27, "relations": [{"object": "681255", "name": "on"}, {"object": "681255", "name": "inside"}], "w": 24, "attributes": ["small", "brown", "delicious"], "y": 123, "x": 68}, "681260": {"name": "rice", "h": 57, "relations": [{"object": "681255", "name": "on"}, {"object": "681258", "name": "to the left of"}], "w": 93, "attributes": ["piled", "white"], "y": 162, "x": 57}, "681269": {"name": "onions", "h": 16, "relations": [], "w": 24, "attributes": ["green"], "y": 147, "x": 90}, "681268": {"name": "tablecloth", "h": 374, "relations": [{"object": "681262", "name": "to the left of"}], "w": 396, "attributes": ["white"], "y": 0, "x": 0}, "681258": {"name": "bowl", "h": 99, "relations": [{"object": "681255", "name": "next to"}, {"object": "681257", "name": "of"}, {"object": "681255", "name": "near"}, {"object": "681256", "name": "to the right of"}, {"object": "681260", "name": "to the right of"}, {"object": "681255", "name": "to the right of"}], "w": 115, "attributes": ["full"], "y": 184, "x": 178}, "681259": {"name": "plantains", "h": 70, "relations": [{"object": "681264", "name": "to the right of"}], "w": 45, "attributes": ["red"], "y": 0, "x": 346}, "681256": {"name": "spoon", "h": 65, "relations": [{"object": "681255", "name": "on"}, {"object": "681257", "name": "to the left of"}, {"object": "681255", "name": "in"}, {"object": "681258", "name": "to the left of"}], "w": 140, "attributes": ["large", "metal", "silver"], "y": 196, "x": 0}, "681257": {"name": "dish", "h": 81, "relations": [{"object": "681258", "name": "inside"}, {"object": "681256", "name": "to the right of"}, {"object": "681258", "name": "in"}, {"object": "681255", "name": "to the right of"}], "w": 108, "attributes": ["cream colored"], "y": 199, "x": 187}, "681254": {"name": "meal", "h": 111, "relations": [], "w": 130, "attributes": [], "y": 121, "x": 58}, "681255": {"name": "plate", "h": 138, "relations": [{"object": "681257", "name": "to the left of"}, {"object": "681254", "name": "of"}, {"object": "681254", "name": "with"}, {"object": "681258", "name": "near"}, {"object": "681258", "name": "to the left of"}], "w": 176, "attributes": ["white", "full"], "y": 111, "x": 30}, "681253": {"name": "banana", "h": 30, "relations": [{"object": "681262", "name": "to the left of"}], "w": 73, "attributes": ["small", "yellow"], "y": 87, "x": 237}}, "height": 375}
% \end{verbatim}

\section{Scene Graph Module}
\label{section:scene_graph_module}

{\color{red} TODO: Verify this still holds: Interestingly, using non-linearities such as ReLU between GAT convolutions destroys the rich information provided by the GloVe embeddings by setting negative values to zero.}

\section{Reasoning Module}
\label{section:reasoning_module}

{\color{red}Partly chose MAC network since is is already well-evaluated on the GQA dataset.}

Given the goal of the question module is to learn a dense contextual and syntactic representation of the question, and the goal of the scene graph module is to capture dependency information between objects, attributes and relations in the scene, we still need to combine and reason about this extracted information. All VQA models have to handle this multi-modal fusion and reasoning in some capacity, with adopting taking fusion-only approaches, whilst others learn self-attention or bidirectional attention weights between question and image modalities, and others leverage the strengths of both methods. For my model, I leverage the reasoning capabilities of the Compositional Attention Network \citeauthor{hudson2018compositional}, a recurrent model that decomposes complex question-answering problems into discrete reasoning steps, each of which is performed by a single cell in the network. The Compositional Attention Network achieved state-of-the-art results on the CLEVR dataset in \citeyear{hudson2018compositional}, achieving 

In order to leverage the computational benefits of sparse tensor operations implemented in PyG \cite{fey2019fast}, I used a PyTorch \cite{paszke2019pytorch} re-implementation of the MAC network, which has been trained to 98.6\% on the CLEVR dataset \cite{eyzaguirre2020differentiable}, just 0.3\% shy of the official result reported by 
\citeauthor{hudson2018compositional}. Notably, this re-implementation accounts for minor details that were omitted from the original MAC network paper but enabled by default in the official MAC network repository {\color{red} citation required}.



\section{Output Module}
\label{section:output_module}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=\textwidth]{output_module.png}
    \caption{An overview of the output module, as implemented in the original P}
    \label{fig:output_module}
\end{figure}

{\color{red}

Move to Ablations

\begin{itemize}
  \item Concat + linear fusion (other fusion types?)
  \item Bottom-up
  \item MAC network
\end{itemize}

\subsection{Bottom-up}
\label{subsection:bottom_up}
 ReLU proved to yield higher results over gated tanh when paired with GAT/GCN embeddings. In the original paper, CNN/R-CNN features are extracted in the preprocessing step, meaning there is no need for gradient propagation to the knowledge base embedding. Preliminary tests showed a need for learnable embeddings in graph convolutional models, and thus a need for end-to-end propagation of gradients.}



\begin{itemize}
  \item Initial tests showed little performance difference between conditioning the current control state on previous control states. % #TODOInvestigate interpretability effects
\end{itemize}
