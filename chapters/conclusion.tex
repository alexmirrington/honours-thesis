\chapter{Conclusion}
\label{chapter:conclusion}

To conclude, I review the primary outcomes of each chapter in this dissertation, summarising how they relate to the visual question answering (VQA) research community, from the literature that inspired my model architecture through to final results and contributions made to the field, and potential directions for future research.

In \chapterautorefname{ \ref{chapter:literature}}, I dissected the VQA problem into three main areas of interest, namely dataset collection, model design and metric creation. I explored methods developed by various VQA dataset creators to collect and/or generate questions and images in large quantities, whilst mitigating the problematic language biases that make effective evaluation of VQA models difficult. I investigated the promising results of recent graph-based models for graph representation learning and VQA, motivated by the lack of current research regarding VQA on pre-annotated scene graphs. Finally, I summarised the types of metrics used for the evaluation of both multiple-choice and open-ended VQA tasks.

Armed with this knowledge, I introduced the methodology behind my VQA model in \chapterautorefname{ \ref{chapter:methodology}}, dividing it into four main components. Together, the question processing module, scene graph processing module, reasoning module and output module form my VQA architecture. I demonstrated the efficiency of my proposed scene graph transformation method, describing it in terms of \(\rho\), the ratio of relations to objects in a scene graph. This analysis makes my VQA architecture extremely portable, as it can draw its scene graph information from any source, whether it be pre-annotated scene graphs like those reported in in this dissertation, or generated scene graphs. Moreover, when using generated scene graphs, as a visual input signal, we have complete control over \(\rho\), allowing my model to be tuned for either performance or efficiency.

In \chapterautorefname{ \ref{chapter:evaluation}}, I introduced relevant details about the GQA dataset and the baseline VQA models used for performance evaluation. To promote a fair comparison between my model and baseline models, I split the models into two groups: those that have access to pre-annotated GQA scene graphs at evaluation time, and those that don't. I compared the performance of my architecture against these baselines in \chapterautorefname{ \ref{chapter:results}}, where it outperformed all other models across all reported metrics. In this comparison, I also demonstrated the importance of a reliable semantic visual signal for VQA tasks; both my own model and the Mutual and Self-Attention (MSA) model outperformed models LXMERT \cite{tang2019learning}, BAN \cite{kim2018bilinear} and GRN \cite{guo2019bilinear} by over 26.0\% and 16.9\% respectively, since the other models rely upon traditional Faster R-CNN \cite{ren2016faster} and object bounding box features. I expect that as scene graph generation methods improve in the coming years, this gap will tighten, as generated scene graphs will be able to capture similar amounts of semantic information as pre-annotated scene graphs. In addition, I performed extensive ablation studies to demonstrate the effectiveness of Graph Attention Networks (GAT) for capturing semantic scene graph features. Finally, I provided a brief overview of the parameter optimisation process that I used to tune my model: over 160 days of GPU computation were used for both reported and non-reported models, and the parameter optimisation process alone required 55 days of GPU computation, indicating the large amount of effort that was required to develop my final model.

Finally, I analysed the reasoning processes used by my model to arrive at an answer to a question and a scene graph, for both correct examples and incorrect examples. Most importantly, we saw that the control and read units of my model's reasoning module attended to semantically similar concepts at each reasoning step, indicating my model's ability to interpret, understand and operate on question and scene graph information.

\section{Future Work}

As briefly mentioned earlier, the primary area for research stemming from results reported in this dissertation is scene graph generation. Whilst we have seen useful scene graph generation models emerge in the last few years \cite{yang2018graph, tang2019learning}, they still perform relatively poorly when evaluated using standard top-k recall measures. The results presented in this dissertation are representative of the kind of performance we could expect from relatively simple graph-based VQA models given quality scene graph information. The model I present is interpretable, efficient and accurate, but still requires an accurate scene graph generation method to be properly used in production contexts.

Although my scene graph transformation and embedding method has a formal guarantee on its worst case complexity if we know \(\rho\), I suspect that there is still ample room for optimisations on top of my proposed transformation method. More specifically, I propose that further research into sparse graph-based models that can handle both node and edge convolutions will greatly increase the quality of the scene graph embeddings demonstrated in my model. As it stands, the GAT weights in my model's scene graph processing module often incorporate object information into the relation nodes and vice-versa. I posit that adopting an approach that separates edge and node convolutions but still allows node and edge embeddings to interact would improve upon my model's interpretability and may result in increased performance.
