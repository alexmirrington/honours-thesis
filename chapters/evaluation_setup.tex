\chapter{Evaluation Setup}

\section{Datasets}

{\color{red} Link to preprocessed data?}

\section{Implementation Details}

Rigorous initial tests were performed - reported and non-reported results represent 159 days of GPU computation time.



\section{Baselines}

\textbf{Compositional Attention Networks (MAC) \cite{hudson2018compositional}}

\textbf{LXMERT \cite{tan2019lxmert}}

\textbf{Neural State Machine (NSM) \cite{hudson2019learning}}

\textbf{TRRNet} \cite{yangtrrnet}
\textbf{Hypergraph Attention Networks} \cite{kim2020hypergraph}

\begin{table}[htbp]
    \begin{footnotesize}
        \begin{tabularx}{\linewidth}{@{}lCccCcCC@{}}
            \toprule
            \multirow{2}{*}{\textbf{Model}} & \multicolumn{7}{c}{GQA Test-standard} \\
            \cmidrule{2-8}
            & Accuracy & Binary & Open & Consistency & Validity & Plausibility & Distribution \\
            \midrule
            Human \cite{hudson2019gqa} & 89.30 & 91.20 & 87.40 & 98.40 & 98.90 & 97.20 & - \\
            \midrule
            Global Prior \cite{hudson2019gqa} & 28.90& 42.94 & 16.62 & 51.69 & 88.86 & 74.81 & 93.08\\
            Local Prior \cite{hudson2019gqa} & 31.24 & 47.90 & 16.66 & 54.04 & 84.33 & 84.31 & 13.98\\
            LSTM \cite{hudson2019gqa} & 41.07 & 61.90 & 22.69 & 68.68 & 96.39 & 87.30 & 17.93\\
            CNN \cite{hudson2019gqa} & 17.82 & 36.05 & 1.74 & 62.40 & 35.78 & 34.84 & 19.99\\
            LSTM + CNN \cite{hudson2019gqa} & 46.55 & 63.26 & 31.80 & 74.57 & 96.02 & 84.25 & 7.46\\
            \midrule
            Bottom-Up \cite{anderson2018bottom} & 49.74 & 66.64 & 34.83 & 78.71 & 96.18 & 84.57 & 5.98\\
            MAC \cite{hudson2018compositional} & 54.06 & 71.23 & 38.91 & 81.59 & 96.16 & 84.48 & 5.34\\
            BAN \cite{kim2018bilinear} & 57.10 & 76.00 & 40.41 & 91.70 & 96.16 & 85.58 & 10.52\\
            LXMERT \cite{tan2019lxmert} & 60.33 & 77.16 & 45.47 & 89.59 & 96.35 & 84.53 & 5.69\\ % val 59.80%, 60.00% test-dev
            GRN \cite{guo2019bilinear} & 61.22 & 78.69 & 45.81 & 90.31 & 96.36 & 85.43 & 6.77\\
            NSM \cite{hudson2019learning} & 63.17 & 78.94 & 49.25 & 93.25 & 96.41 & 84.28 & 3.71\\
            TRRNet\(^*\) \cite{yangtrrnet} & 63.20 & 77.91 & 50.22 & 89.84 & 96.47 & 85.15 & 5.25 \\
            \midrule
            LXMERT\(^\dagger\) \cite{tan2019lxmert} & 62.71 & 79.79 & 47.64 & 93.1 & 96.36 & 85.21 & 6.42\\
            HAN\(^{\dagger*}\)\cite{kim2020hypergraph} & 73.33 & 79.68 & 67.73 & 77.02 & 96.36 & 83.70 & 2.46\\
            TRRNet\(^{\dagger*}\) \cite{yangtrrnet} & 74.03 & 82.12 & 66.89 & 89.00 & 96.76 & 83.58 & 1.29\\
            \bottomrule
        \end{tabularx}
        \caption{A comparison of various models on the GQA test set for multiple metrics, including top models from the GQA challenge leaderboard \cite{gqachallenge} for which results have been formally published. Models marked with a \(^*\) use scene graph and/or functional program annotations during training, and those marked with a \(^\dagger\) are ensemble models.}
    \end{footnotesize}
\end{table}

% \begin{table}[htbp]
%     \begin{footnotesize}
%         \begin{tabularx}{\linewidth}{@{}lccCcCCC@{}}
%             \toprule
%             \multirow{2}{*}{\textbf{Model}} & \multicolumn{7}{c}{GQA Validation} \\
%             \cmidrule{2-8}
%             & Binary & Open & Consistency & Validity & Plausibility & Distribution & Accuracy \\
%             \midrule
%             Human \cite{hudson2019gqa} & 91.20 & 87.40 & 98.40 & 98.90 & 97.20 & - & 89.30\\
%             \midrule
%             LXMERT \cite{tan2019lxmert, tan2019lxmertgithub} & - & - & - & - & - & - & 59.80\\
%             % - & - & - & - & - & - & - & -\\
%             \bottomrule
%         \end{tabularx}
%         \caption{A comparison of various models on the GQA validation set for multiple metrics.}
%     \end{footnotesize}
% \end{table}

